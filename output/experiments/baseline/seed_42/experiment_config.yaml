data:
  enable_thinking: false
  max_length: 512
  test_file: data/processed/test_data.jsonl
  train_file: data/processed/train_full_sft.jsonl
experiment:
  description: Standard LoRA fine-tuning with cross-entropy loss only
  name: baseline
  random_seed: 42
  type: baseline
lora:
  alpha: 16
  bias: none
  dropout: 0.1
  rank: 16
  target_modules:
  - q_proj
  - k_proj
  - v_proj
  - o_proj
  - gate_proj
  - up_proj
  - down_proj
  task_type: CAUSAL_LM
model:
  name: Qwen3-8B
  path: ./
  quantization:
    bnb_4bit_compute_dtype: bfloat16
    bnb_4bit_quant_type: nf4
    bnb_4bit_use_double_quant: true
    load_in_4bit: true
output:
  logging_dir: ./output/experiments/baseline/seed_42/logs
  output_dir: ./output/experiments/baseline/seed_42
training:
  bf16: true
  early_stopping:
    enabled: true
    patience: 3
    threshold: 0.01
  eval_steps: 50
  eval_strategy: steps
  gradient_accumulation_steps: 4
  gradient_checkpointing: true
  greater_is_better: false
  learning_rate: 0.0002
  load_best_model_at_end: true
  logging_steps: 10
  lr_scheduler_type: cosine
  max_grad_norm: 1.0
  metric_for_best_model: eval_loss
  num_epochs: 2
  optim: adamw_torch
  per_device_eval_batch_size: 4
  per_device_train_batch_size: 4
  save_steps: 50
  save_total_limit: 3
  warmup_ratio: 0.03
  weight_decay: 0.01
wandb:
  enabled: true
  name: baseline-seed42
  project: qwen3-twitter-polychromic
  tags:
  - baseline
  - seed42
