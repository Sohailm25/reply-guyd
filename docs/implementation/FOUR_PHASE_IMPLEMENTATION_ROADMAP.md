# üöÄ Four-Phase Implementation Roadmap
## Strategic Plan for Maximum Scientific Impact

**Created:** October 3, 2025  
**Purpose:** Complete the research implementation with high-impact scientific contributions  
**Status:** Ready to Execute  
**Based On:** Novelty analysis and competitive positioning strategy

---

## üìä Implementation Status Matrix

| Component | Phase | Priority | Status | Breaks Code? | Effort |
|-----------|-------|----------|--------|--------------|--------|
| **Phase 1: Core Results (Current Focus)** |
| SFT Training (Baseline) | 1 | P0 | ‚úÖ Done | No | - |
| SFT Training (Polychromic) | 1 | P0 | ‚úÖ Done | No | - |
| Basic Evaluation Metrics | 1 | P0 | ‚úÖ Done | No | - |
| Four-Baseline Framework | 1 | P1 | ‚úÖ Done | No | - |
| **Diversity Dynamics Tracking** | 1 | **P0** | ‚ùå TODO | No | 4 hrs |
| **Pareto Frontier Visualization** | 1 | **P1** | ‚ùå TODO | No | 2 hrs |
| **Phase 2: Theoretical Depth** |
| GRPO Trainer Implementation | 2 | P0 | ‚ùå TODO | No | 3 days |
| Reward Model Implementation | 2 | P0 | ‚ùå TODO | No | 2 days |
| **LoRA Parameter Analysis** | 2 | **P1** | ‚ùå TODO | No | 1 day |
| **Mechanistic Interpretation** | 2 | **P2** | ‚ùå TODO | No | 2 days |
| **Theoretical Analysis Utils** | 2 | **P2** | ‚ùå TODO | No | 1 day |
| **Phase 3: Generalization** |
| **Code Domain Evaluation** | 3 | **P0** | ‚ùå TODO | No | 3 days |
| **Creative Writing Evaluation** | 3 | **P1** | ‚ùå TODO | No | 2 days |
| Cross-Domain Analysis | 3 | P1 | ‚ùå TODO | No | 1 day |
| **Phase 4: Novel Metrics** |
| **User Selection Quality (USQ)** | 4 | **P0** | ‚ùå TODO | No | 1 day |
| **Diversity Efficiency Ratio** | 4 | **P1** | ‚ùå TODO | No | 4 hrs |
| **Collapse Point Analysis** | 4 | **P2** | ‚ùå TODO | No | 4 hrs |

**Legend:**
- P0 = Critical for publication
- P1 = High impact, strongly recommended
- P2 = Nice-to-have, adds depth

---

## üéØ PHASE 1: Core Results + Quick Wins (Week 1-2)

### **Goal:** Strengthen current experiments without breaking existing code

### **1.1 Diversity Dynamics Tracking** ‚≠ê HIGHEST PRIORITY
**Why:** Novel analysis not in any paper, provides mechanistic understanding

**What to Implement:**
```python
# File: src/evaluation/diversity_dynamics.py (NEW)

class DiversityDynamicsTracker:
    """Track how diversity evolves during training."""
    
    def __init__(self, model, tokenizer, validation_prompts, n_samples=10):
        self.model = model
        self.tokenizer = tokenizer
        self.validation_prompts = validation_prompts
        self.n_samples = n_samples
        self.history = []
    
    def compute_at_step(self, step):
        """Compute diversity metrics at current training step."""
        diversity_metrics = {
            'step': step,
            'self_bleu': compute_self_bleu(...),
            'distinct_2': compute_distinct_n(..., n=2),
            'semantic_diversity': compute_semantic_diversity(...),
            'entropy': compute_output_entropy(...),
        }
        self.history.append(diversity_metrics)
        return diversity_metrics
    
    def plot_trajectory(self, save_path):
        """Generate diversity trajectory plot."""
        # Create publication-quality figure
        pass
```

**Integration Points:**
- Add to `base_trainer.py` line ~150 (in `on_step_end` callback)
- Add to `polychromic_trainer.py` line ~200 (in `compute_loss`)
- NO CHANGES to core training logic

**Config Addition:**
```yaml
# Add to all experiment configs
evaluation:
  track_dynamics: true
  dynamics_frequency: 100  # Every 100 steps
  dynamics_prompts: 20     # Use 20 validation prompts
```

**Deliverable:**
- `diversity_trajectory_plot.pdf` showing SFT vs GRPO phases
- `dynamics_data.json` with all measurements
- **Paper Figure 2:** "Diversity Dynamics Through Training"

---

### **1.2 Pareto Frontier Visualization**
**Why:** Standard multi-objective optimization visualization

**What to Implement:**
```python
# File: src/evaluation/pareto_analysis.py (NEW)

def compute_pareto_frontier(models: List[str], metrics_dir: str):
    """
    Compute Pareto frontier for quality vs. diversity trade-off.
    
    Returns:
        pareto_points: List of non-dominated (quality, diversity) pairs
        model_mapping: Which model corresponds to each point
    """
    points = []
    for model in models:
        quality = load_metric(model, 'rouge_l')
        diversity = load_metric(model, 'semantic_diversity')
        points.append((quality, diversity, model))
    
    # Identify Pareto frontier
    pareto = find_pareto_optimal(points)
    return pareto

def plot_pareto_frontier(pareto_points, all_points, save_path):
    """Publication-quality Pareto frontier plot."""
    plt.figure(figsize=(8, 6))
    # Plot all points
    # Highlight Pareto frontier
    # Add model labels
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
```

**Integration Points:**
- Add to `scripts/analysis/visualize_results.py`
- Called AFTER all experiments complete
- NO CHANGES to training or evaluation code

**Deliverable:**
- `pareto_frontier.pdf`
- **Paper Figure 3:** "Quality-Diversity Trade-off Pareto Frontier"

---

## üß™ PHASE 2: Theoretical Depth (Week 3-4)

### **Goal:** Add GRPO training + mechanistic understanding

### **2.1 GRPO Trainer Implementation** ‚≠ê CRITICAL
**Why:** Needed for two-phase training experiments

**Status:** Documented but not coded

**What to Implement:**
```python
# File: src/training/grpo_trainer.py (NEW)
# Based on docs/research/grpo_implementation_strategy.md

class GRPOTrainer(Trainer):
    """Group Relative Policy Optimization trainer."""
    
    def __init__(
        self,
        reward_function,
        reference_model,
        n_generations=4,
        kl_coeff=0.1,
        *args, **kwargs
    ):
        super().__init__(*args, **kwargs)
        # Implementation from documentation
        pass
    
    def compute_loss(self, model, inputs, return_outputs=False):
        """
        GRPO loss = -E[log_prob * advantage] + Œ≤ * KL
        """
        # Generate K responses
        # Score with reward model
        # Compute group advantages
        # Policy gradient + KL penalty
        pass
```

**Integration Points:**
- Add to `scripts/training/train_model.py`:
  ```python
  if config.training.get('use_grpo', False):
      from src.training.grpo_trainer import GRPOTrainer
      trainer = GRPOTrainer(...)
  ```
- Use existing configs: `grpo_from_baseline.yaml`, `grpo_from_polychromic.yaml`
- NO CHANGES to existing SFT trainers

**Dependencies:**
- Requires reward model (2.2)
- Can use heuristic reward as fallback

**Deliverable:**
- Working GRPO trainer
- 2 additional models: Baseline‚ÜíGRPO, Polychromic‚ÜíGRPO

---

### **2.2 Reward Model Implementation**
**Why:** Needed for GRPO training

**What to Implement:**
```python
# File: src/training/reward_model.py (NEW)

class EngagementRewardModel(nn.Module):
    """Predict engagement score from tweet + reply."""
    
    def __init__(self, base_model="sentence-transformers/all-MiniLM-L6-v2"):
        super().__init__()
        self.encoder = AutoModel.from_pretrained(base_model)
        self.regressor = nn.Sequential(
            nn.Linear(768, 256),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, 1),
            nn.Sigmoid()
        )
    
    def forward(self, tweet_embeds, reply_embeds):
        combined = torch.cat([tweet_embeds, reply_embeds], dim=-1)
        score = self.regressor(combined)
        return score
    
    def compute_reward(self, tweet, reply):
        """Compute single reward for a tweet/reply pair."""
        pass
```

**Training Script:**
```python
# File: scripts/training/train_reward_model.py (NEW)

def train_reward_model(data_path, output_dir, epochs=10):
    """
    Train reward model on engagement scores.
    
    Uses actual like counts as supervision signal.
    """
    pass
```

**Integration Points:**
- Called BEFORE GRPO training
- Used by GRPOTrainer
- Independent of SFT training

**Deliverable:**
- Trained reward model
- Validation metrics (correlation > 0.5 with actual engagement)

---

### **2.3 LoRA Parameter Analysis** ‚≠ê HIGH IMPACT
**Why:** First analysis of WHERE diversity lives in parameter space

**What to Implement:**
```python
# File: src/evaluation/lora_analysis.py (NEW)

def analyze_lora_parameters(model_path, layer_names):
    """
    Analyze LoRA matrices to understand where diversity is encoded.
    
    Metrics:
    - ||ŒîW_A||, ||ŒîW_B|| per layer
    - Effective rank of ŒîW = B¬∑A
    - Singular values distribution
    - Layer-wise contribution to output diversity
    """
    pass

def compare_lora_updates(baseline_path, polychromic_path):
    """
    Compare how baseline vs polychromic training affects different layers.
    """
    pass

def visualize_lora_heatmap(analysis_results, save_path):
    """
    Heatmap showing which layers change most.
    """
    pass
```

**Integration Points:**
- Run AFTER training completes
- Standalone analysis script
- NO CHANGES to training code

**Deliverable:**
- `lora_parameter_analysis.pdf`
- **Paper Figure 4:** "Layer-wise LoRA Parameter Analysis"
- Insight: "Diversity primarily affects layers 24-36"

---

### **2.4 Mechanistic Interpretation** (Optional but High Impact)

**What to Implement:**
```python
# File: src/evaluation/mechanistic_analysis.py (NEW)

def analyze_attention_patterns(model, prompts):
    """Extract and compare attention patterns."""
    pass

def analyze_hidden_states(model, prompts, layer=20):
    """Cluster hidden states for diverse generations."""
    pass

def analyze_gradient_flow(trainer):
    """Track gradient norms during GRPO."""
    pass
```

**Deliverable:**
- Supplementary material
- Deep understanding of mechanism

---

## üåç PHASE 3: Cross-Domain Generalization (Week 5-6)

### **Goal:** Show method works beyond social media

### **3.1 Code Generation Evaluation** ‚≠ê CRITICAL
**Why:** Direct comparison with Pass@k Training paper

**What to Implement:**
```python
# File: src/evaluation/code_evaluation.py (NEW)

class CodeDomainEvaluator:
    """Evaluate on HumanEval or MBPP dataset."""
    
    def __init__(self, dataset="HumanEval"):
        self.dataset = self.load_dataset(dataset)
    
    def evaluate_model(self, model, k_values=[1, 5, 10]):
        """
        Standard Pass@k evaluation for code.
        
        Uses unit tests as ground truth (objective evaluation).
        """
        pass
    
    def compute_code_diversity(self, solutions):
        """Code-specific diversity metrics (AST edit distance)."""
        pass
```

**Data:**
- Use HumanEval (164 problems, public)
- Or MBPP (974 problems, public)
- NO NEW DATA COLLECTION NEEDED

**Integration Points:**
- New evaluation script: `scripts/evaluation/evaluate_code_domain.py`
- Reuse existing models (no retraining needed)
- Just evaluate on different domain

**Deliverable:**
- Pass@k results on code generation
- Direct comparison: "While Pass@k Training achieves X, we achieve Y with LoRA"
- **Paper Table 3:** "Cross-Domain Generalization Results"

---

### **3.2 Creative Writing Evaluation**
**Why:** Shows generalization to subjective, open-ended tasks

**What to Implement:**
```python
# File: src/evaluation/creative_evaluation.py (NEW)

class CreativeWritingEvaluator:
    """Evaluate on story continuation or poetry generation."""
    
    def __init__(self, dataset="WritingPrompts"):
        self.dataset = self.load_dataset(dataset)
    
    def evaluate_creativity(self, generations):
        """
        Metrics:
        - Diversity (semantic, lexical)
        - Coherence (LLM-as-judge)
        - Engagement (human ratings if available)
        """
        pass
```

**Data:**
- Use WritingPrompts dataset (public)
- Or use synthetic prompts

**Deliverable:**
- Demonstrates method works for creative tasks
- **Paper Section 5.3:** "Generalization to Creative Tasks"

---

## üìä PHASE 4: Novel Metrics (Week 7)

### **Goal:** Introduce new evaluation methodology

### **4.1 User Selection Quality (USQ) Metric** ‚≠ê REVOLUTIONARY
**Why:** More realistic than standard Pass@k

**What to Implement:**
```python
# File: src/evaluation/usq_metric.py (NEW)

def compute_usq(model, prompts, k=10, selector='reward_model'):
    """
    User Selection Quality: Quality of best-selected from k candidates.
    
    Args:
        model: Model to evaluate
        prompts: Test prompts
        k: Number of candidates to generate
        selector: How to select best ('reward_model', 'length', 'diversity')
    
    Returns:
        mean_quality: Average quality of selected responses
    """
    scores = []
    for prompt in prompts:
        # Generate k candidates
        candidates = model.generate(prompt, num_return_sequences=k)
        
        # Select best using selector function
        selected = select_best(candidates, selector)
        
        # Measure quality of selected
        quality = evaluate_quality(selected)
        scores.append(quality)
    
    return np.mean(scores)
```

**Key Insight:**
```
Standard Pass@k: "Best of k according to ground truth"
USQ: "Best of k according to realistic selection mechanism"

USQ is what actually happens in deployment!
```

**Integration Points:**
- Add to `scripts/evaluation/evaluate_comprehensive.py`
- Compare USQ vs Pass@k across models

**Deliverable:**
- **Paper Section 4.3:** "User Selection Quality: A Deployment-Aware Metric"
- Show Polychromic‚ÜíGRPO has highest USQ
- **Novel contribution to evaluation methodology**

---

### **4.2 Diversity Efficiency Ratio (DER)**

**What to Implement:**
```python
# File: src/evaluation/usq_metric.py (extend)

def compute_der(passk_curve):
    """
    Diversity Efficiency Ratio: How much does diversity help?
    
    DER = Pass@k / (k √ó Pass@1)
    
    Perfect diverse model: DER = 1.0
    Mode-collapsed: DER ‚Üí 0
    """
    pass_1 = passk_curve[1]
    der_values = {}
    for k, pass_k in passk_curve.items():
        der = pass_k / (k * pass_1) if pass_1 > 0 else 0
        der_values[k] = der
    return der_values
```

**Deliverable:**
- Single-number summary of diversity benefit
- Compare DER across all models

---

### **4.3 Collapse Point Analysis**

**What to Implement:**
```python
def find_collapse_point(passk_curve, threshold=0.01):
    """
    Find k where Pass@k stops improving significantly.
    
    Tells you "how many candidates are useful before diminishing returns"
    """
    k_values = sorted(passk_curve.keys())
    derivatives = np.gradient([passk_curve[k] for k in k_values])
    
    collapse_idx = np.argmax(derivatives < threshold)
    return k_values[collapse_idx]
```

**Deliverable:**
- Practical insight: "Generate 8 candidates for Polychromic, only 3 for Baseline"
- **Computational cost guidance**

---

## üóÇÔ∏è File Structure After Implementation

```
src/
‚îú‚îÄ‚îÄ training/
‚îÇ   ‚îú‚îÄ‚îÄ base_trainer.py                 ‚úÖ Existing
‚îÇ   ‚îú‚îÄ‚îÄ polychromic_trainer.py          ‚úÖ Existing
‚îÇ   ‚îú‚îÄ‚îÄ grpo_trainer.py                 üÜï Phase 2
‚îÇ   ‚îú‚îÄ‚îÄ reward_model.py                 üÜï Phase 2
‚îÇ   ‚îî‚îÄ‚îÄ data_module.py                  ‚úÖ Existing
‚îÇ
‚îú‚îÄ‚îÄ evaluation/
‚îÇ   ‚îú‚îÄ‚îÄ diversity_metrics.py            ‚úÖ Existing
‚îÇ   ‚îú‚îÄ‚îÄ quality_metrics.py              ‚úÖ Existing
‚îÇ   ‚îú‚îÄ‚îÄ statistical_tests.py            ‚úÖ Existing
‚îÇ   ‚îú‚îÄ‚îÄ llm_judge.py                    ‚úÖ Existing
‚îÇ   ‚îú‚îÄ‚îÄ passk_evaluation.py             ‚úÖ Existing
‚îÇ   ‚îú‚îÄ‚îÄ diversity_dynamics.py           üÜï Phase 1
‚îÇ   ‚îú‚îÄ‚îÄ pareto_analysis.py              üÜï Phase 1
‚îÇ   ‚îú‚îÄ‚îÄ lora_analysis.py                üÜï Phase 2
‚îÇ   ‚îú‚îÄ‚îÄ mechanistic_analysis.py         üÜï Phase 2 (optional)
‚îÇ   ‚îú‚îÄ‚îÄ code_evaluation.py              üÜï Phase 3
‚îÇ   ‚îú‚îÄ‚îÄ creative_evaluation.py          üÜï Phase 3 (optional)
‚îÇ   ‚îî‚îÄ‚îÄ usq_metric.py                   üÜï Phase 4
‚îÇ
‚îî‚îÄ‚îÄ utils/
    ‚îî‚îÄ‚îÄ theoretical_utils.py            üÜï Phase 2 (optional)

scripts/
‚îú‚îÄ‚îÄ training/
‚îÇ   ‚îú‚îÄ‚îÄ train_model.py                  ‚úÖ Existing (modify for GRPO)
‚îÇ   ‚îú‚îÄ‚îÄ train_reward_model.py           üÜï Phase 2
‚îÇ   ‚îî‚îÄ‚îÄ validate_reward_model.py        üÜï Phase 2
‚îÇ
‚îú‚îÄ‚îÄ evaluation/
‚îÇ   ‚îú‚îÄ‚îÄ evaluate_comprehensive.py       ‚úÖ Existing (extend)
‚îÇ   ‚îú‚îÄ‚îÄ evaluate_code_domain.py         üÜï Phase 3
‚îÇ   ‚îî‚îÄ‚îÄ evaluate_creative_domain.py     üÜï Phase 3 (optional)
‚îÇ
‚îî‚îÄ‚îÄ analysis/
    ‚îú‚îÄ‚îÄ visualize_results.py            ‚úÖ Existing (extend)
    ‚îú‚îÄ‚îÄ analyze_lora_parameters.py      üÜï Phase 2
    ‚îî‚îÄ‚îÄ analyze_dynamics.py             üÜï Phase 1
```

---

## ‚öôÔ∏è Implementation Strategy: Non-Breaking Changes

### **Principle: Extension, Not Modification**

1. **New files only** - Don't modify existing working code
2. **Optional features** - Add flags to enable new functionality
3. **Backward compatible** - Existing configs still work
4. **Incremental testing** - Test each addition independently

### **Safe Integration Pattern**

```python
# Example: Adding diversity dynamics to trainer

# DON'T: Modify base_trainer.py directly
# DO: Add optional callback

class DiversityDynamicsCallback(TrainerCallback):
    """Optional callback for tracking diversity dynamics."""
    
    def on_step_end(self, args, state, control, **kwargs):
        if state.global_step % self.tracking_frequency == 0:
            # Track diversity metrics
            pass

# In train_model.py:
callbacks = []
if config.get('track_dynamics', False):
    callbacks.append(DiversityDynamicsCallback(...))

trainer = Trainer(..., callbacks=callbacks)
```

### **Testing Strategy**

For each new component:
1. Write unit test
2. Test in isolation
3. Test with existing code
4. Verify no regression in baseline experiments

---

## üìÖ Realistic Timeline

### **Minimal Viable Publication (MVP)**

**Week 1-2: Phase 1 (Quick Wins)**
- ‚úÖ Day 1-2: Implement diversity dynamics tracking
- ‚úÖ Day 3: Implement Pareto frontier visualization
- ‚úÖ Day 4-5: Test and integrate

**Week 3-4: Phase 2 (GRPO)**
- ‚úÖ Week 3: Implement reward model + GRPO trainer
- ‚úÖ Week 4: Train 2 GRPO models (Baseline‚ÜíGRPO, Polychromic‚ÜíGRPO)

**Week 5: Phase 3 (Code Domain)**
- ‚úÖ Day 1-2: Implement code evaluation
- ‚úÖ Day 3-4: Run experiments on HumanEval
- ‚úÖ Day 5: Analyze results

**Week 6: Phase 4 (Novel Metrics)**
- ‚úÖ Day 1-2: Implement USQ metric
- ‚úÖ Day 3: Implement DER and collapse point
- ‚úÖ Day 4-5: Regenerate all results with new metrics

**Week 7-8: Analysis & Writing**
- ‚úÖ Generate all figures
- ‚úÖ Statistical analysis
- ‚úÖ Write paper

**Total: 8 weeks to publication-ready**

### **Extended Version (Maximum Impact)**

Add 2-4 more weeks for:
- Multi-seed training (3 seeds)
- Creative writing domain
- Mechanistic interpretation
- Theoretical analysis section

---

## üí∞ Cost Estimate

| Phase | Component | GPU Time | Cost |
|-------|-----------|----------|------|
| **Phase 1** | Dynamics tracking | 0 (passive) | $0 |
| **Phase 1** | Pareto visualization | 0 (post-hoc) | $0 |
| **Phase 2** | Reward model training | 2 hrs | $2 |
| **Phase 2** | GRPO training (2 models) | 28 hrs | $22 |
| **Phase 2** | LoRA analysis | 0 (post-hoc) | $0 |
| **Phase 3** | Code evaluation | 4 hrs | $0 (inference only) |
| **Phase 3** | Creative evaluation | 2 hrs | $0 (inference only) |
| **Phase 4** | USQ evaluation | 0 (post-hoc) | $0 |
| **LLM-as-Judge** | All evaluations | - | $120 |
| **Multi-seed** | 3 seeds (optional) | +64 hrs | +$50 |
| **TOTAL (MVP)** | | 34 hrs | **$144** |
| **TOTAL (Extended)** | | 98 hrs | **$194** |

**Affordable for a strong publication!**

---

## ‚úÖ Success Criteria

### **Minimal Success (Publishable at Workshop)**
- ‚úÖ Phase 1 complete (dynamics + Pareto)
- ‚úÖ Phase 2 complete (GRPO working)
- ‚úÖ Statistical significance (p < 0.05)
- ‚úÖ Polychromic‚ÜíGRPO best Pass@10

### **Strong Success (Arxiv-worthy)**
- ‚úÖ All of minimal
- ‚úÖ Phase 3 complete (code domain)
- ‚úÖ Phase 4 complete (USQ metric)
- ‚úÖ Multi-seed results
- ‚úÖ Effect size d > 0.5

### **Exceptional Success (Top-Tier Venue)**
- ‚úÖ All of strong
- ‚úÖ Creative writing domain
- ‚úÖ Mechanistic interpretation
- ‚úÖ Theoretical analysis
- ‚úÖ Novel positioning as "training-deployment alignment"

---

## üöÄ Getting Started

### **Today (Day 0):**
1. ‚úÖ Read this document
2. ‚úÖ Review current codebase status
3. ‚úÖ Understand novelty positioning from `novelty_research_claude.md`

### **Tomorrow (Day 1):**
1. Create branch: `git checkout -b feature/diversity-dynamics`
2. Implement `src/evaluation/diversity_dynamics.py`
3. Add optional callback to trainer
4. Test on 100 training steps

### **This Week (Days 2-5):**
1. Complete Phase 1 implementations
2. Test thoroughly
3. Generate first diversity dynamics plots
4. Merge to main

### **Next Week:**
1. Start Phase 2 (GRPO)
2. Follow implementation plan

---

## üéØ Key Takeaways

**What This Plan Achieves:**
1. ‚úÖ Strengthens novelty claims
2. ‚úÖ Differentiates from Pass@k Training & BA-LoRA
3. ‚úÖ Adds theoretical depth
4. ‚úÖ Shows cross-domain generalization
5. ‚úÖ Introduces novel metrics (USQ)
6. ‚úÖ Maintains existing code integrity

**Why This Will Succeed:**
- Incremental, non-breaking changes
- Realistic timeline and budget
- High-impact additions
- Strong scientific positioning
- Publication-ready deliverables

**Critical Success Factors:**
1. Complete Phase 1 quickly (1 week) - builds momentum
2. GRPO must work well - core contribution
3. Code domain results - shows generalization
4. USQ metric - novel evaluation contribution

---

## üìû Next Steps Decision Points

**After Phase 1 (Week 2):**
- **Continue to Phase 2?** If dynamics plots look good ‚Üí YES
- **Skip GRPO?** Only if major technical barriers ‚Üí Publish SFT-only

**After Phase 2 (Week 4):**
- **Continue to Phase 3?** If GRPO works well ‚Üí YES
- **Add more seeds?** If results look promising ‚Üí YES

**After Phase 3 (Week 6):**
- **Continue to Phase 4?** If time allows ‚Üí YES
- **Start writing?** If results are strong ‚Üí YES

**Flexibility:** Can stop after any phase and still have publishable work

---

**This roadmap transforms good research into exceptional research. Let's execute systematically! üöÄ**

