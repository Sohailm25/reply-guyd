# Hyperparameter Strategy for Small Datasets

## ğŸ¯ **Decision: Start Standard, Adjust if Needed**

### **Recommended Approach**

**Phase 1: Run with current configs (rank=16, Î»=0.3)**
- Learn actual overfitting patterns
- Understand quality-diversity tradeoff
- Get baseline performance
- Cost: ~$12

**Phase 2: Evaluate results**
- Check train/eval loss gap
- Examine W&B curves
- Review sample outputs

**Phase 3: Decide next step**
- If working well â†’ Multiple seeds
- If overfitting â†’ Conservative configs
- If underfitting â†’ More aggressive configs

---

## ğŸ“Š **Current vs Conservative Configs**

### Standard Configuration (Use First)

**File:** `config/experiments/baseline.yaml` & `polychromic_0.3.yaml`

```yaml
lora:
  rank: 16
  alpha: 16
  dropout: 0.1

polychromic:
  diversity_weight: 0.3
  n_generations: 3

training:
  learning_rate: 2.0e-4
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1
```

**Rationale:**
- Research-informed (QLoRA paper recommendations)
- Gives us baseline to learn from
- Early stopping protects against severe overfitting
- Statistical comparison is cleaner

---

### Conservative Configuration (Use If Overfitting)

**File:** `config/experiments/baseline_conservative.yaml` & `polychromic_conservative.yaml`

```yaml
lora:
  rank: 8              # â†“ from 16 - fewer parameters
  alpha: 16            # Keep 2x rank ratio
  dropout: 0.15        # â†‘ from 0.1 - more regularization

polychromic:
  diversity_weight: 0.15  # â†“ from 0.3 - prioritize quality
  n_generations: 2        # â†“ from 3 - faster, less diversity pressure

training:
  learning_rate: 1.0e-4   # â†“ from 2e-4 - safer convergence
  train_split: 0.7        # â†“ from 0.8 - more eval data
  val_split: 0.15         # â†‘ from 0.1 - better early stopping signal
  test_split: 0.15        # â†‘ from 0.1 - more robust testing
```

**Use when:**
- Train/eval loss gap > 0.4
- Eval loss starts increasing after epoch 1
- Model memorizing training examples
- Validation perplexity increasing

---

## ğŸš¨ **Overfitting Detection Guide**

### **Signs to Watch in W&B:**

**ğŸŸ¢ Healthy Training:**
```
Epoch 1: train_loss=0.8 â†’ eval_loss=0.85
Epoch 2: train_loss=0.6 â†’ eval_loss=0.65
Gap: 0.05 âœ“ Good!
```

**ğŸŸ¡ Mild Overfitting (Acceptable):**
```
Epoch 1: train_loss=0.8 â†’ eval_loss=0.9
Epoch 2: train_loss=0.5 â†’ eval_loss=0.7
Gap: 0.2 âš ï¸ Borderline, early stopping will help
```

**ğŸ”´ Severe Overfitting (Use Conservative):**
```
Epoch 1: train_loss=0.8 â†’ eval_loss=1.0
Epoch 2: train_loss=0.3 â†’ eval_loss=1.2 â†‘
Gap: 0.9 âŒ Stop and use conservative configs
```

### **What to Check:**

1. **Training Curves (W&B)**
   - `train/loss` should decrease smoothly
   - `eval/loss` should track close to train
   - Gap < 0.3 is healthy

2. **Sample Outputs**
   - Are they generic/repetitive? â†’ Overfitting
   - Do they vary appropriately? â†’ Good
   - Are they on-topic? â†’ Check quality

3. **Perplexity**
   - Should be reasonable (5-20)
   - If skyrocketing â†’ Problem

---

## ğŸ“ **When to Use Each Configuration**

### **Scenario 1: Unknown Dataset Size (Current)**
â†’ **Use Standard configs first**
- We don't know final size yet (collection ongoing)
- Standard gives us learning baseline
- Can adjust after seeing results

### **Scenario 2: Final Dataset < 800 pairs**
â†’ **Consider Conservative immediately**
- Very small dataset
- Start with rank=8
- Lower learning rate

### **Scenario 3: Final Dataset 800-1,500 pairs**
â†’ **Use Standard, monitor closely**
- Should work with rank=16
- Early stopping will protect
- Adjust if gap > 0.3

### **Scenario 4: Final Dataset > 1,500 pairs**
â†’ **Standard is perfect**
- Enough data for rank=16
- May even consider rank=32 in ablations

---

## ğŸ“ **Decision Matrix**

| Observed Behavior | Action |
|-------------------|--------|
| Train/eval gap < 0.2 | âœ… Continue with standard, try multiple seeds |
| Train/eval gap 0.2-0.4 | âš ï¸ Monitor, early stopping will handle it |
| Train/eval gap > 0.4 | âŒ Stop, switch to conservative configs |
| Eval loss increasing | âŒ Overfitting, use conservative |
| Good diversity, poor quality | ğŸ”„ Reduce diversity_weight to 0.15 |
| Good quality, poor diversity | ğŸ”„ Keep or increase diversity_weight |
| Training too slow | â© Reduce n_generations from 3â†’2 |

---

## ğŸ’¡ **Recommended Workflow**

### **Week 1: Initial Training**

```bash
# After data curation complete

# 1. Train baseline (standard config)
python scripts/training/train_model.py \
  --config config/experiments/baseline.yaml \
  --data data/processed/training_data.jsonl

# Monitor in W&B for 1-2 hours
# Check: Is train/eval gap reasonable?
```

### **Decision Point 1: After Baseline Epoch 1**

**If train/eval gap < 0.3:**
```bash
# Continue baseline, start polychromic (standard)
python scripts/training/train_model.py \
  --config config/experiments/polychromic_0.3.yaml \
  --data data/processed/training_data.jsonl
```

**If train/eval gap > 0.4:**
```bash
# Stop baseline, restart with conservative
python scripts/training/train_model.py \
  --config config/experiments/baseline_conservative.yaml \
  --data data/processed/training_data.jsonl
```

### **Week 2: Analysis & Iteration**

After both models trained:
1. Run comprehensive evaluation
2. Analyze results
3. Decide on ablations or multiple seeds

---

## ğŸ”¬ **Research Perspective**

### **Why Standard First is Better for Research:**

1. **Cleaner Comparison**
   - Baseline vs Polychromic with same hyperparams
   - Isolates effect of diversity objective
   - More interpretable results

2. **Learning Opportunity**
   - See actual overfitting patterns
   - Understand quality-diversity tradeoff
   - Inform future work

3. **Publishable Either Way**
   - Good results â†’ Great!
   - Overfitting â†’ "We found rank=16 too high for N<1000"
   - Both are valid contributions

4. **Cost is Minimal**
   - $12 for first run
   - $12 for conservative run if needed
   - Total $24 to explore both strategies

---

## âœ… **Immediate Action Items**

### **Now (Before Training):**
- [ ] Wait for data collection to complete
- [ ] Manual curation to 800-1,200 pairs
- [ ] Note actual dataset size

### **Decision Point:**
- [ ] If dataset < 800 pairs â†’ Use conservative configs
- [ ] If dataset 800-1,500 pairs â†’ Use standard configs first
- [ ] If dataset > 1,500 pairs â†’ Definitely use standard

### **During Training:**
- [ ] Monitor W&B dashboard closely
- [ ] Check train/eval gap after epoch 1
- [ ] Be ready to stop and switch if severe overfitting

### **After First Run:**
- [ ] Evaluate results
- [ ] Decide: continue with standard, switch to conservative, or proceed to multiple seeds
- [ ] Document findings

---

## ğŸ“š **Configuration Files Reference**

**Standard (Use First):**
- `config/experiments/baseline.yaml`
- `config/experiments/polychromic_0.3.yaml`

**Conservative (Use If Overfitting):**
- `config/experiments/baseline_conservative.yaml`
- `config/experiments/polychromic_conservative.yaml`

**All configs ready to use!**

---

## ğŸ¯ **Final Recommendation**

**Start with standard configs (rank=16, Î»=0.3).**

**Reasons:**
1. âœ… Don't know final dataset size yet
2. âœ… Research-informed baseline
3. âœ… Early stopping protects us
4. âœ… Learn more from first run
5. âœ… Cost is minimal ($12)
6. âœ… Conservative configs ready if needed

**Switch to conservative ONLY if:**
- Train/eval gap > 0.4
- Eval loss increases
- Severe overfitting observed

**You're set up for success either way!** ğŸš€

