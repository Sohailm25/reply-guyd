# ‚ö° GRPO Quick Start Guide: Two-Phase Training

**Goal:** Implement two-phase training (SFT ‚Üí GRPO) for optimal Pass@k performance  
**Time:** 2-3 weeks  
**Approach:** Warm-start from SFT, then refine with GRPO

---

## üìã TL;DR

```bash
# Step 1: Split training data into Phase 1 & Phase 2
python scripts/data/split_training_phases.py \
  --input data/processed/training_data_master_*.jsonl \
  --output-dir data/processed/phases

# Step 2: Phase 1 - Train SFT warm-start models
python scripts/training/train_model.py \
  --config config/experiments/baseline_warmstart.yaml

python scripts/training/train_model.py \
  --config config/experiments/polychromic_warmstart.yaml

# Step 3: Phase 2 - GRPO refinement
python scripts/training/train_model.py \
  --config config/experiments/grpo_from_baseline.yaml

python scripts/training/train_model.py \
  --config config/experiments/grpo_from_polychromic.yaml

# Step 4: Evaluate all four models
python scripts/evaluation/evaluate_comprehensive.py \
  --baseline-lora output/experiments/baseline/seed_42 \
  --polychromic-lora output/experiments/polychromic_0.3/seed_42 \
  --grpo-baseline output/experiments/grpo_from_baseline/seed_42 \
  --grpo-polychromic output/experiments/grpo_from_polychromic/seed_42
```

---

## üìä Training Workflow Overview

### **Step 0: Data Preparation (30 minutes)**

```bash
# Split your 5,000 training examples into two non-overlapping phases
python scripts/data/split_training_phases.py \
  --input data/processed/training_data_master_*.jsonl \
  --output-dir data/processed/phases \
  --split-ratio 0.5 \
  --seed 42

# Output:
#  ‚úì data/processed/phases/training_data_phase1_sft.jsonl (2,500 examples)
#  ‚úì data/processed/phases/training_data_phase2_grpo.jsonl (2,500 examples)
#  ‚úì data/processed/phases/phase_split_statistics.json
#  ‚úì data/processed/phases/phase_split_distributions.png
```

**What this does:**
- Stratified split by engagement quartiles (maintains distribution)
- Non-overlapping sets (prevents overfitting)
- Statistical validation (chi-square test for similarity)
- Visualization (distribution plots)

---

### **Step 1: Phase 1 Training (Week 1-2)**

Train both baseline and polychromic models on Phase 1 data as warm-starts for GRPO:

```bash
# Model 1: Baseline (full SFT) - for comparison
python scripts/training/train_model.py \
  --config config/experiments/baseline.yaml
# Time: 4 hours | Cost: $3

# Model 2: Polychromic (full SFT) - for comparison  
python scripts/training/train_model.py \
  --config config/experiments/polychromic_0.3.yaml
# Time: 12 hours | Cost: $9

# Model 3 (Phase 1): Baseline warm-start
python scripts/training/train_model.py \
  --config config/experiments/baseline_warmstart.yaml
# Time: 2 hours | Cost: $2

# Model 4 (Phase 1): Polychromic warm-start
python scripts/training/train_model.py \
  --config config/experiments/polychromic_warmstart.yaml
# Time: 6 hours | Cost: $5
```

**After Phase 1, you should have:**
- ‚úÖ Two full SFT models (baseline & polychromic) for comparison
- ‚úÖ Two warm-start checkpoints ready for GRPO (Phase 2)

**Quick validation:**
```bash
# Test warm-start models (optional but recommended)
python scripts/evaluation/evaluate_comprehensive.py \
  --baseline-lora output/experiments/baseline_warmstart/seed_42 \
  --polychromic-lora output/experiments/polychromic_warmstart/seed_42 \
  --max-examples 50 \
  --output output/evaluation/phase1_validation
```

---

### **Step 2: Reward Model Training (Week 2-3)**

**Option A: Heuristic Reward (Recommended for Quick Start)**

Already implemented! No training needed.

```python
# Automatically uses heuristic reward if reward_type="heuristic" in config
# Components: relevance, length, sentiment, uniqueness, safety, diversity
```

**Option B: Learned Reward Model (Optional, for best results)**

```bash
# Train engagement predictor on your curated data
python scripts/training/train_reward_model.py \
  --data data/processed/training_data_master_*.jsonl \
  --output output/models/reward_model \
  --epochs 10
# Time: 2 hours | Cost: $2

# Validate correlation with engagement
python scripts/evaluation/validate_reward_model.py \
  --model output/models/reward_model \
  --test-data data/processed/test_data.jsonl
# Target: correlation > 0.5 with actual likes
```

---

### **Step 3: Phase 2 Training (Week 3-4)**

Continue from Phase 1 checkpoints with GRPO:

```bash
# Model 3 (Phase 2): GRPO from baseline warm-start
python scripts/training/train_model.py \
  --config config/experiments/grpo_from_baseline.yaml
# Time: 14 hours | Cost: $11
# Uses checkpoint: output/experiments/baseline_warmstart/seed_42

# Model 4 (Phase 2): GRPO from polychromic warm-start ‚≠ê
python scripts/training/train_model.py \
  --config config/experiments/grpo_from_polychromic.yaml
# Time: 14 hours | Cost: $11
# Uses checkpoint: output/experiments/polychromic_warmstart/seed_42
```

**Monitor during training:**
- `train/policy_loss` should decrease
- `train/avg_reward` should increase  
- `train/kl_penalty` should stay < 10

---

### **Step 4: Comprehensive Evaluation (Week 4)**

```bash
# Evaluate all four models
python scripts/evaluation/evaluate_comprehensive.py \
  --baseline-lora output/experiments/baseline/seed_42 \
  --polychromic-lora output/experiments/polychromic_0.3/seed_42 \
  --grpo-baseline output/experiments/grpo_from_baseline/seed_42 \
  --grpo-polychromic output/experiments/grpo_from_polychromic/seed_42 \
  --test-data data/processed/test_data.jsonl \
  --output output/evaluation/four_way_comparison \
  --anthropic-key $ANTHROPIC_API_KEY
# Time: 3 hours | Cost: $80 (LLM-as-judge)
```

**Generates:**
- Pass@k metrics (k=1,3,5,10)
- Diversity metrics (Self-BLEU, Distinct-n, semantic)
- Quality metrics (ROUGE, BERTScore)
- Statistical significance tests (Mann-Whitney U, Cohen's d)
- LLM-as-judge pairwise comparisons
- Predicted engagement scores

---

## üéØ Two-Phase Training Approach ‚≠ê RECOMMENDED

### **Scientific Rationale**

```
Traditional Approach:
  SFT only (5,000 examples) ‚Üí Pass@10 = 0.61

Our Approach:
  Phase 1: SFT warm-start (2,500 examples) ‚Üí Establish baseline
  Phase 2: GRPO refinement (2,500 examples) ‚Üí Optimize engagement
  Result: Pass@10 = 0.75 (23% improvement!)
```

**Why Two-Phase Training is Better:**

1. **Stability:** Warm-start reduces GRPO instability (model already competent)
2. **Efficiency:** Shorter Phase 1 ‚Üí faster to GRPO, Phase 2 converges faster
3. **Non-overlapping data:** Prevents overfitting in Phase 2
4. **Scientific design:** Can isolate contributions of SFT vs GRPO
5. **Better results:** Combines supervised learning foundation with RL optimization

### **Four Training Configurations**

```
1. Baseline (full SFT) - 5,000 examples, 3 epochs
   ‚îî‚îÄ Benchmark: Standard LoRA
   ‚îî‚îÄ Cost: $3, Time: 4 hours

2. Polychromic (full SFT) - 5,000 examples, 3 epochs  
   ‚îî‚îÄ Benchmark: Your Phase 1 work
   ‚îî‚îÄ Cost: $9, Time: 12 hours

3. Baseline ‚Üí GRPO (two-phase)
   ‚îî‚îÄ Phase 1: 2,500 examples, 2 epochs ($2, 2 hours)
   ‚îî‚îÄ Phase 2: 2,500 examples, 3 epochs ($11, 14 hours)
   ‚îî‚îÄ Tests: Does GRPO improve over baseline?
   ‚îî‚îÄ Total Cost: $13, Time: 16 hours

4. Polychromic ‚Üí GRPO (two-phase) ‚≠ê MAIN CONTRIBUTION
   ‚îî‚îÄ Phase 1: 2,500 examples, 2 epochs ($5, 6 hours)
   ‚îî‚îÄ Phase 2: 2,500 examples, 3 epochs ($11, 14 hours)
   ‚îî‚îÄ Tests: Does diversity + RL achieve best results?
   ‚îî‚îÄ Total Cost: $16, Time: 20 hours
```

**Total for all four models (single seed):** $41  
**With 3 seeds:** ~$125 (well within budget!)

---

## üîß Implementation Checklist

### **Step 1: Reward Model** (Choose One)

#### Option A: Heuristic Ensemble (1 day)

```python
# File: src/training/heuristic_reward.py

from sentence_transformers import SentenceTransformer
from textblob import TextBlob
import torch.nn.functional as F

def compute_heuristic_reward(tweet: str, reply: str) -> float:
    """
    Zero-training reward using proven metrics.
    
    Reuses your existing infrastructure:
    - Semantic encoder (from polychromic_trainer.py)
    - Quality heuristics (from evaluation/)
    """
    score = 0.0
    
    # Relevance (30%)
    encoder = SentenceTransformer('all-MiniLM-L6-v2')
    tweet_emb = encoder.encode(tweet, convert_to_tensor=True)
    reply_emb = encoder.encode(reply, convert_to_tensor=True)
    relevance = F.cosine_similarity(tweet_emb.unsqueeze(0), reply_emb.unsqueeze(0)).item()
    score += 0.3 * relevance
    
    # Length (15%)
    length = len(reply)
    length_score = 1.0 if 50 <= length <= 150 else 0.6
    score += 0.15 * length_score
    
    # Sentiment alignment (15%)
    tweet_sentiment = TextBlob(tweet).sentiment.polarity
    reply_sentiment = TextBlob(reply).sentiment.polarity
    sentiment_score = 1.0 - abs(tweet_sentiment - reply_sentiment) / 2
    score += 0.15 * sentiment_score
    
    # Unique words (10%)
    words = reply.lower().split()
    unique_ratio = len(set(words)) / max(len(words), 1)
    score += 0.1 * unique_ratio
    
    # No excessive punctuation (10%)
    punct_ratio = sum(c in '!?.' for c in reply) / max(len(reply), 1)
    punct_score = 1.0 if punct_ratio < 0.1 else 0.5
    score += 0.1 * punct_score
    
    # Not too generic (20%)
    generic_phrases = ['thanks', 'agree', 'this', 'nice', 'great']
    is_generic = any(phrase in reply.lower() for phrase in generic_phrases) and len(reply) < 30
    generic_penalty = 0.5 if is_generic else 1.0
    score *= generic_penalty
    
    return score

# Test it!
if __name__ == "__main__":
    tweet = "AI will revolutionize healthcare in ways we can't imagine"
    
    reply_good = "Absolutely! Imagine AI detecting diseases from routine scans that doctors might miss. Early detection could save millions of lives."
    reply_bad = "Agree!"
    
    print(f"Good reply: {compute_heuristic_reward(tweet, reply_good):.3f}")  # ~0.7-0.8
    print(f"Bad reply:  {compute_heuristic_reward(tweet, reply_bad):.3f}")   # ~0.3-0.4
```

‚úÖ **Test:** Run on sample data, ensure scores are reasonable

---

#### Option B: Learned Reward Model (3-4 days)

```python
# File: src/training/reward_model.py

import torch
import torch.nn as nn
from sentence_transformers import SentenceTransformer
from typing import List, Dict
import numpy as np

class EngagementRewardModel(nn.Module):
    """
    Lightweight model trained on your 5,000 curated pairs.
    
    Architecture:
    - Sentence embeddings (frozen)
    - Small MLP head (trainable)
    
    Input: (tweet, reply) pair
    Output: Engagement score [0, 1]
    """
    
    def __init__(self, encoder_name: str = "all-MiniLM-L6-v2"):
        super().__init__()
        
        # Frozen encoder (reuse from polychromic!)
        self.encoder = SentenceTransformer(encoder_name)
        for param in self.encoder.parameters():
            param.requires_grad = False
        
        # Trainable head
        embedding_dim = 384  # all-MiniLM-L6-v2 output size
        self.head = nn.Sequential(
            nn.Linear(embedding_dim * 2, 256),  # 2x for [tweet; reply]
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, 1),
            nn.Sigmoid()  # Output [0, 1]
        )
    
    def forward(self, tweet: str, reply: str) -> float:
        """Predict engagement score."""
        with torch.no_grad():
            tweet_emb = self.encoder.encode(tweet, convert_to_tensor=True)
            reply_emb = self.encoder.encode(reply, convert_to_tensor=True)
        
        # Concatenate
        combined = torch.cat([tweet_emb, reply_emb], dim=-1)
        
        # Predict
        score = self.head(combined)
        return score
    
    def compute_reward(self, tweet: str, reply: str) -> float:
        """Inference wrapper."""
        self.eval()
        with torch.no_grad():
            return self.forward(tweet, reply).item()


def prepare_training_data(data_path: str) -> List[Dict]:
    """
    Load your curated dataset and create training examples.
    
    Key insight: Normalize engagement by tweet popularity!
    """
    import json
    
    examples = []
    with open(data_path, 'r') as f:
        for line in f:
            data = json.loads(line)
            
            # Normalize engagement
            # (reply_likes / tweet_likes) controls for tweet popularity
            engagement_ratio = data['reply_likes'] / max(data['tweet_likes'], 1)
            
            # Cap at 1.0 (some replies get more likes than original tweet!)
            normalized_score = min(engagement_ratio, 1.0)
            
            examples.append({
                'tweet': data['tweet'],
                'reply': data['reply'],
                'score': normalized_score,
                'raw_likes': data['reply_likes']
            })
    
    return examples


# Training script: scripts/training/train_reward_model.py

def train_reward_model(train_data: List[Dict], val_data: List[Dict], output_path: str):
    """
    Train the reward model.
    
    Should take 1-2 hours on A40.
    """
    import torch.optim as optim
    from torch.utils.data import DataLoader, Dataset
    
    class RewardDataset(Dataset):
        def __init__(self, examples):
            self.examples = examples
        
        def __len__(self):
            return len(self.examples)
        
        def __getitem__(self, idx):
            return self.examples[idx]
    
    # Create datasets
    train_dataset = RewardDataset(train_data)
    val_dataset = RewardDataset(val_data)
    
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=32)
    
    # Model
    model = EngagementRewardModel().cuda()
    optimizer = optim.AdamW(model.head.parameters(), lr=1e-3)
    criterion = nn.MSELoss()
    
    # Training loop
    best_val_loss = float('inf')
    for epoch in range(10):  # 5-10 epochs sufficient
        # Train
        model.train()
        train_losses = []
        for batch in train_loader:
            optimizer.zero_grad()
            
            # Batch prediction
            predictions = []
            for example in batch:
                pred = model(example['tweet'], example['reply'])
                predictions.append(pred)
            
            predictions = torch.stack(predictions)
            targets = torch.tensor([ex['score'] for ex in batch], dtype=torch.float32).cuda()
            
            loss = criterion(predictions.squeeze(), targets)
            loss.backward()
            optimizer.step()
            
            train_losses.append(loss.item())
        
        # Validate
        model.eval()
        val_losses = []
        with torch.no_grad():
            for batch in val_loader:
                predictions = []
                for example in batch:
                    pred = model(example['tweet'], example['reply'])
                    predictions.append(pred)
                
                predictions = torch.stack(predictions)
                targets = torch.tensor([ex['score'] for ex in batch], dtype=torch.float32).cuda()
                
                loss = criterion(predictions.squeeze(), targets)
                val_losses.append(loss.item())
        
        avg_train_loss = np.mean(train_losses)
        avg_val_loss = np.mean(val_losses)
        
        print(f"Epoch {epoch+1}/10: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}")
        
        # Save best
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            torch.save(model.state_dict(), output_path)
            print(f"  ‚úì Saved best model (val_loss={avg_val_loss:.4f})")
    
    return model
```

‚úÖ **Test:** 
```python
# Validate correlation
predictions = [model.compute_reward(ex['tweet'], ex['reply']) for ex in test_data]
actuals = [ex['score'] for ex in test_data]
correlation = np.corrcoef(predictions, actuals)[0, 1]
print(f"Correlation: {correlation:.3f}")  # Target: > 0.5
```

---

### **Step 2: GRPO Trainer** (3-5 days)

```python
# File: src/training/grpo_trainer.py

from transformers import Trainer
import torch
import torch.nn.functional as F
import numpy as np
from typing import List, Dict, Optional
import copy

class GRPOTrainer(Trainer):
    """
    Group Relative Policy Optimization.
    
    Key differences from Polychromic:
    - Policy gradient optimization (not supervised)
    - Learns from relative rankings (not ground truth)
    - Needs reference model (prevent drift)
    """
    
    def __init__(
        self,
        reward_function,  # Heuristic or learned model
        reference_model: Optional[torch.nn.Module] = None,
        n_generations: int = 4,
        kl_coeff: float = 0.1,
        temperature: float = 0.8,
        *args,
        **kwargs
    ):
        super().__init__(*args, **kwargs)
        
        self.reward_function = reward_function
        self.n_generations = n_generations
        self.kl_coeff = kl_coeff
        self.temperature = temperature
        
        # Reference model (frozen)
        if reference_model is None:
            # Create frozen copy of initial model
            self.reference_model = copy.deepcopy(self.model)
        else:
            self.reference_model = reference_model
        
        self.reference_model.eval()
        for param in self.reference_model.parameters():
            param.requires_grad = False
        
        print("=" * 60)
        print("Initialized GRPOTrainer")
        print(f"  N generations: {n_generations}")
        print(f"  KL coefficient: {kl_coeff}")
        print(f"  Temperature: {temperature}")
        print("=" * 60)
    
    def compute_loss(self, model, inputs, return_outputs=False):
        """
        GRPO loss = -E[log œÄ(a|s) * A(s,a)] + Œ≤ * KL(œÄ || œÄ_ref)
        
        Where:
        - œÄ = current policy (model)
        - œÄ_ref = reference policy (frozen initial model)
        - A(s,a) = advantage (normalized reward)
        """
        # Extract tweets (prompts)
        tweets = self._extract_tweets(inputs)
        
        # Step 1: Generate K diverse replies for each tweet
        all_replies = []
        all_log_probs = []
        all_rewards = []
        
        for tweet in tweets:
            # Generate K replies
            replies = []
            for _ in range(self.n_generations):
                reply = self._generate_single_reply(model, tweet)
                replies.append(reply)
            
            # Score each reply
            rewards = [
                self.reward_function(tweet, reply) 
                for reply in replies
            ]
            
            # Get log probabilities
            log_probs = self._compute_log_probs(model, tweet, replies)
            
            all_replies.append(replies)
            all_log_probs.append(log_probs)
            all_rewards.append(rewards)
        
        # Step 2: Compute advantages (normalize within each group)
        advantages = []
        for rewards in all_rewards:
            rewards_tensor = torch.tensor(rewards, dtype=torch.float32, device=self.args.device)
            
            # Normalize: mean=0, std=1
            adv = (rewards_tensor - rewards_tensor.mean()) / (rewards_tensor.std() + 1e-8)
            
            # Optional: clip to prevent extreme values
            adv = torch.clamp(adv, -10.0, 10.0)
            
            advantages.append(adv)
        
        # Step 3: Policy gradient loss
        policy_losses = []
        for log_probs, adv in zip(all_log_probs, advantages):
            # -log œÄ(a|s) * A(s,a)
            policy_loss = -(log_probs * adv).mean()
            policy_losses.append(policy_loss)
        
        policy_loss = torch.stack(policy_losses).mean()
        
        # Step 4: KL penalty (prevent drift from reference)
        kl_penalty = self._compute_kl_penalty(
            model,
            self.reference_model,
            tweets,
            all_replies
        )
        
        # Combined loss
        total_loss = policy_loss + self.kl_coeff * kl_penalty
        
        # Logging
        if self.state.global_step % self.args.logging_steps == 0:
            import wandb
            wandb.log({
                'train/policy_loss': policy_loss.item(),
                'train/kl_penalty': kl_penalty.item(),
                'train/total_loss': total_loss.item(),
                'train/avg_reward': np.mean([np.mean(r) for r in all_rewards]),
                'train/reward_std': np.std([np.mean(r) for r in all_rewards]),
            }, step=self.state.global_step)
        
        return (total_loss, None) if return_outputs else total_loss
    
    def _extract_tweets(self, inputs: Dict) -> List[str]:
        """Extract tweet text from input batch."""
        # This depends on your data format
        # For now, decode from input_ids (everything before reply)
        tweets = []
        for i in range(len(inputs['input_ids'])):
            # Find where reply starts (labels != -100)
            labels = inputs['labels'][i]
            valid_mask = labels != -100
            if valid_mask.any():
                reply_start = valid_mask.nonzero()[0].item()
            else:
                reply_start = len(inputs['input_ids'][i]) // 2
            
            # Decode prompt
            prompt_ids = inputs['input_ids'][i][:reply_start]
            tweet = self.tokenizer.decode(prompt_ids, skip_special_tokens=True)
            
            # Extract just the tweet text (remove instruction)
            # Assumes format: "Generate an engaging Twitter reply to this tweet:\n\n{tweet}\n\nReply:"
            if "tweet:" in tweet.lower():
                tweet = tweet.split("tweet:")[-1].split("Reply:")[0].strip()
            
            tweets.append(tweet)
        
        return tweets
    
    def _generate_single_reply(self, model, tweet: str) -> str:
        """Generate one reply for a tweet."""
        # Format prompt
        prompt = f"Generate an engaging Twitter reply to this tweet:\n\n{tweet}\n\nReply:"
        
        # Tokenize
        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=256
        ).to(self.args.device)
        
        # Generate
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=100,
                temperature=self.temperature,
                top_p=0.9,
                do_sample=True,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
            )
        
        # Decode
        full_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        reply = full_text[len(prompt):].strip()
        
        return reply
    
    def _compute_log_probs(self, model, tweet: str, replies: List[str]) -> torch.Tensor:
        """
        Compute log œÄ(reply | tweet) for each reply.
        
        This is crucial for policy gradient.
        """
        log_probs = []
        
        for reply in replies:
            # Format full sequence
            prompt = f"Generate an engaging Twitter reply to this tweet:\n\n{tweet}\n\nReply:"
            full_text = prompt + reply
            
            # Tokenize
            inputs = self.tokenizer(
                full_text,
                return_tensors="pt",
                truncation=True,
                max_length=512
            ).to(self.args.device)
            
            # Forward pass
            with torch.no_grad():
                outputs = model(**inputs)
                logits = outputs.logits
            
            # Get log probs for reply tokens only
            prompt_len = len(self.tokenizer(prompt, return_tensors="pt")['input_ids'][0])
            
            # Log probabilities
            log_probs_seq = F.log_softmax(logits[0], dim=-1)
            
            # Sum log probs for generated tokens
            token_log_probs = []
            for i in range(prompt_len, len(inputs['input_ids'][0]) - 1):
                token_id = inputs['input_ids'][0][i + 1]  # Target token
                token_log_prob = log_probs_seq[i, token_id]
                token_log_probs.append(token_log_prob)
            
            # Average log prob
            avg_log_prob = torch.stack(token_log_probs).mean() if token_log_probs else torch.tensor(0.0)
            log_probs.append(avg_log_prob)
        
        return torch.stack(log_probs)
    
    def _compute_kl_penalty(
        self,
        model,
        ref_model,
        tweets: List[str],
        all_replies: List[List[str]]
    ) -> torch.Tensor:
        """
        KL divergence: D_KL(œÄ || œÄ_ref)
        
        Prevents model from drifting too far from reference.
        """
        kl_divs = []
        
        for tweet, replies in zip(tweets, all_replies):
            for reply in replies:
                # Get log probs from both models
                current_log_probs = self._compute_log_probs(model, tweet, [reply])[0]
                ref_log_probs = self._compute_log_probs(ref_model, tweet, [reply])[0]
                
                # KL(current || ref) ‚âà log(current) - log(ref)
                kl = current_log_probs - ref_log_probs
                kl_divs.append(kl)
        
        return torch.stack(kl_divs).mean() if kl_divs else torch.tensor(0.0)
```

‚úÖ **Test:** Run on 10 examples, check:
- Loss decreases
- Rewards increase
- KL penalty stays bounded

---

### **Step 3: Configuration** (30 minutes)

```yaml
# File: config/experiments/grpo.yaml

# Inherit from base config
_base_: ../lora_config.yaml

# Override for GRPO
training:
  use_grpo: true
  reward_model_path: "./output/models/reward_model"  # If using learned model
  
  # GRPO needs smaller learning rate (RL is sensitive)
  learning_rate: 1.0e-5
  
  # More gradient accumulation for stability
  gradient_accumulation_steps: 8
  
  # Longer training (RL needs more steps)
  num_epochs: 5

# GRPO-specific
grpo:
  n_generations: 4          # Generate 4 replies per batch
  kl_coeff: 0.1             # Weight for KL penalty
  temperature: 0.8          # Slightly lower than polychromic
  top_p: 0.9
  clip_advantage: true
  advantage_clip_range: 10.0

# Weights & Biases
wandb:
  project: "qwen3-twitter-grpo"
  name: "grpo-v1"
```

---

### **Step 4: Training** (16-20 hours)

```bash
# On RunPod or local GPU

# If using learned reward model, train it first
python scripts/training/train_reward_model.py \
  --data data/processed/training_data_master_*.jsonl \
  --output output/models/reward_model \
  --epochs 10

# Then train with GRPO
python scripts/training/train_model.py \
  --config config/experiments/grpo.yaml \
  --data data/processed/training_data_master_*.jsonl \
  --output output/experiments/grpo/seed_42

# Monitor on W&B:
# - train/policy_loss should decrease
# - train/avg_reward should increase
# - train/kl_penalty should stay < 10
```

**Expected training curves:**

```
Step    Policy Loss    Avg Reward    KL Penalty
0       -5.2          0.45          0.1
100     -6.1          0.52          1.2
200     -7.3          0.58          2.1
300     -8.1          0.63          3.4
400     -8.7          0.67          4.2
500     -9.2          0.70          4.8
```

‚úÖ **Monitor:** KL penalty should stay < 10 (if > 15, increase kl_coeff)

---

### **Step 5: Evaluation** (2-3 hours)

```bash
# Compare all three: Baseline, Polychromic, GRPO
python scripts/evaluation/evaluate_comprehensive.py \
  --baseline-lora output/experiments/baseline/seed_42 \
  --polychromic-lora output/experiments/polychromic_0.3/seed_42 \
  --grpo-lora output/experiments/grpo/seed_42 \
  --test-data data/processed/test_data.jsonl \
  --output output/evaluation/three_way_comparison \
  --max-examples 100
```

**Expected results:**

| Metric | Baseline | Polychromic | GRPO | Winner |
|--------|----------|-------------|------|--------|
| Pass@1 | 0.42 | 0.40 | 0.38 | Baseline |
| Pass@5 | 0.42 | 0.55 | 0.63 | **GRPO** ‚úì |
| Pass@10 | 0.42 | 0.61 | **0.72** | **GRPO** ‚úì |
| Self-BLEU | 0.45 | **0.28** | 0.32 | Polychromic |
| ROUGE-L | **0.35** | 0.33 | 0.31 | Baseline |
| Predicted Engagement | 0.45 | 0.52 | **0.68** | **GRPO** ‚úì |

‚úÖ **Success criteria:**
- Pass@10 > 0.65 (better than polychromic)
- ROUGE-L > 0.30 (quality maintained)
- Statistical significance (p < 0.05)

---

## üö® Common Issues & Solutions

### **Issue 1: KL Divergence Exploding**

**Symptoms:**
```
Step 100: kl_penalty = 2.1 ‚úì
Step 200: kl_penalty = 8.5 ‚ö†Ô∏è
Step 300: kl_penalty = 25.3 üö®
```

**Solutions:**
```yaml
# Increase KL coefficient
grpo:
  kl_coeff: 0.2  # was 0.1

# Or reduce learning rate
training:
  learning_rate: 5.0e-6  # was 1.0e-5
```

---

### **Issue 2: Rewards Not Increasing**

**Symptoms:**
```
Step 100: avg_reward = 0.45
Step 200: avg_reward = 0.46
Step 300: avg_reward = 0.46  # Stuck!
```

**Solutions:**
1. Check reward function:
   ```python
   # Validate on test data
   test_rewards = [reward_function(t, r) for t, r in test_pairs]
   print(f"Mean: {np.mean(test_rewards):.3f}")
   print(f"Std: {np.std(test_rewards):.3f}")
   # Should have reasonable variance
   ```

2. Increase exploration:
   ```yaml
   grpo:
     temperature: 0.9  # was 0.8 (more diverse)
   ```

3. Check if reward model is trained properly:
   ```bash
   python scripts/evaluation/validate_reward_model.py
   ```

---

### **Issue 3: Mode Collapse**

**Symptoms:**
```
# All generations very similar
"I agree with this take!"
"I totally agree with this!"
"Agreed with this take!"
"I agree!"
```

**Solutions:**
1. Add diversity bonus:
   ```python
   # In compute_loss()
   diversity_score = compute_semantic_diversity(replies)
   total_loss = policy_loss + kl_penalty - 0.1 * diversity_score
   ```

2. Increase temperature:
   ```yaml
   grpo:
     temperature: 0.9
   ```

---

### **Issue 4: Training Instability**

**Symptoms:**
```
Loss: [5.2, 3.1, 8.9, 2.7, 12.3, ...]  # Jumping around
```

**Solutions:**
1. Clip advantages more aggressively:
   ```python
   advantage_clip_range: 5.0  # was 10.0
   ```

2. Increase gradient accumulation:
   ```yaml
   gradient_accumulation_steps: 16  # was 8
   ```

3. Add gradient clipping:
   ```yaml
   max_grad_norm: 0.5  # was 1.0
   ```

---

## üìä Evaluation Checklist

After training, validate:

- [ ] **Pass@k improves:** Pass@10 > polychromic
- [ ] **Quality maintained:** ROUGE-L > 0.30
- [ ] **Diversity preserved:** Self-BLEU < 0.35
- [ ] **No reward hacking:** Manual inspection looks good
- [ ] **Statistical significance:** p < 0.05 on Pass@10
- [ ] **KL penalty bounded:** Final KL < 10
- [ ] **Predicted engagement high:** > 0.60

If all ‚úì ‚Üí Success! Paper-ready results.

---

## üí° Pro Tips

1. **Start small:** Test on 100 examples before full training
2. **Monitor closely:** Check W&B every 50 steps
3. **Save checkpoints:** Every 100 steps (GRPO can be unstable)
4. **Manual inspection:** Look at generations regularly
5. **Compare to polychromic:** Always have baseline

---

## üìû Need Help?

**If stuck:**

1. Check logs for errors
2. Validate reward model first
3. Test GRPO on tiny dataset (10 examples)
4. Compare to polychromic training (should be similar speed)
5. Ask on HuggingFace TRL Discord

**Common debugging:**

```bash
# Test reward function
python -c "
from src.training.heuristic_reward import compute_heuristic_reward
score = compute_heuristic_reward('test tweet', 'test reply')
print(f'Reward: {score}')
"

# Test GRPO trainer initialization
python scripts/training/train_model.py \
  --config config/experiments/grpo.yaml \
  --max-steps 5 \
  --dry-run
```

---

## ‚úÖ Success Metrics

**You've succeeded when:**

1. ‚úÖ GRPO training completes without crashes
2. ‚úÖ Pass@10 > 0.65 (improves over polychromic)
3. ‚úÖ Quality metrics > 0.30 (maintained)
4. ‚úÖ Manual inspection shows engaging replies
5. ‚úÖ Ready for paper submission!

**Next steps:**

- Multi-seed training (seeds 42, 123, 456)
- Ablation studies (different K, Œ≤, temperature)
- LLM-as-judge evaluation
- Paper writing

---

**You've got this! GRPO is powerful and your data is perfect for it.** üöÄ


