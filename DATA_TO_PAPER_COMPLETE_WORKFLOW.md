# üéØ Complete Workflow: Data Curation ‚Üí Paper Submission

**Your definitive guide from data ready to paper submission**

**Timeline:** 7 weeks total  
**Cost:** $132  
**Outcome:** 5 novel contributions, 6 paper figures, EMNLP 2026 submission

---

## üìã Pre-Flight Checklist

Before starting, ensure:
- [x] Phase 1, 2 (LoRA), 4 implementation complete ‚úÖ
- [ ] Data collection complete (~5,000 raw pairs)
- [ ] Manual curation ready to start
- [ ] GPU access available (RunPod or local)
- [ ] W&B account set up
- [ ] Anthropic API key for LLM-judge

---

## üóìÔ∏è Week-by-Week Workflow

### **Week 1: Data Preparation & Setup**

#### **Day 1-2: Manual Curation (2-3 days)**

**Goal:** Select best 5,000 pairs from collected data

```bash
# Check what you have
wc -l data/raw/training_data_master_*.jsonl

# Review data quality
head -100 data/raw/training_data_master_*.jsonl | jq .

# Manual review (use your judgment):
# - Remove spam, low-quality, irrelevant
# - Keep high-engagement, well-written pairs
# - Ensure author diversity
# - Target: 5,000 high-quality pairs

# Save curated data
cp data/raw/training_data_master_*.jsonl data/processed/training_data_curated_5000.jsonl

# Verify count
wc -l data/processed/training_data_curated_5000.jsonl
# Should show: 5000
```

**Output:** `data/processed/training_data_curated_5000.jsonl`

---

#### **Day 3: Setup Training Environment**

**If using RunPod:**

**Detailed setup: See `docs/runpod-training/RUNPOD_SETUP.md` for complete guide**

**Quick version:**
```bash
# 1. Create RunPod instance
#    - GPU: RTX A6000 (48GB VRAM) ‚≠ê Recommended ($0.39-0.79/hr)
#    - Template: PyTorch 2.1+
#    - Container Disk: 50GB
#    - Volume Disk: 50GB (optional but recommended)

# 2. Connect via SSH
ssh root@<pod-ip> -p <port>
# Port shown in RunPod dashboard

# 3. On RunPod: Clone repo
cd /workspace
git clone <your-repo-url> Qwen3-8
cd Qwen3-8

# 4. Install dependencies
pip install -r requirements-runpod.txt

# 5. Download model (IMPORTANT: Download on pod, DON'T upload 16GB!)
huggingface-cli download Qwen/Qwen3-8B --local-dir ./ --local-dir-use-symlinks False
# This downloads 16GB model directly to pod (fast!)

# 6. Upload ONLY data from Mac (separate terminal on Mac):
rsync -avz -e "ssh -p <port>" \
  data/processed/training_data_curated_5000.jsonl \
  root@<pod-ip>:/workspace/Qwen3-8/data/processed/

# 7. Set up W&B on RunPod
export WANDB_API_KEY=your_key_here
wandb login

# 8. Test installation
python scripts/test_installation.py
# Should see: ‚úì ALL TESTS PASSED
```

**If using local GPU:**
```bash
cd /Users/sohailmo/Repos/experiments/cuda/Qwen3-8

# IMPORTANT: Activate virtual environment
source qwen-lora-env/bin/activate

# Verify you're in venv
which python
# Should show: .../qwen-lora-env/bin/python
# NOT: /usr/bin/python or /opt/homebrew/bin/python

# Test installation
python scripts/test_installation.py
```

**Virtual Environment Troubleshooting:**

If commands fail with "module not found":
```bash
# Check if in venv
which python

# If NOT in venv, activate it:
source qwen-lora-env/bin/activate

# Or use wrapper script:
./run.sh python <command>
```

**See `docs/reference/QUICK_REFERENCE.md` for more venv details.**

---

### **Week 2: SFT Training**

#### **Day 4: Train Baseline Model (4-6 hours)**

```bash
# Start training
python scripts/training/train_model.py \
  --config config/experiments/baseline.yaml \
  --data data/processed/training_data_curated_5000.jsonl \
  --seed 42

# Monitor training:
# - W&B dashboard: Real-time metrics
# - Check: train/loss decreasing
# - Check: eval/loss < 0.5 at convergence
# - Check: diversity_dynamics.json being created every 100 steps

# Expected: 4 hours on A40
# Cost: ~$3
```

**Outputs:**
- `output/experiments/baseline/seed_42/adapter_model.safetensors`
- `output/experiments/baseline/seed_42/diversity_dynamics.json`
- `output/experiments/baseline/seed_42/diversity_trajectory.pdf`
- W&B run with all metrics

**‚úÖ Checkpoint: Baseline trained!**

---

#### **Day 5-6: Train Polychromic Model (10-14 hours)**

```bash
# Start training
python scripts/training/train_model.py \
  --config config/experiments/polychromic_0.3.yaml \
  --data data/processed/training_data_curated_5000.jsonl \
  --seed 42

# Monitor training:
# - Check: train/quality_loss decreasing
# - Check: train/diversity_score > 0.3
# - Check: train/combined_loss decreasing
# - Check: Diversity NOT collapsing (stays > 0.25)

# Expected: 12 hours on A40
# Cost: ~$9
```

**Outputs:**
- `output/experiments/polychromic_0.3/seed_42/adapter_model.safetensors`
- `output/experiments/polychromic_0.3/seed_42/diversity_dynamics.json`
- `output/experiments/polychromic_0.3/seed_42/diversity_trajectory.pdf`
- W&B run with all metrics

**‚úÖ Checkpoint: Polychromic trained!**

---

#### **Day 6.5: Troubleshooting Training Issues (If Needed)**

**‚ö†Ô∏è If training fails or shows poor results:**

**1. Check for overfitting:**
```bash
# View W&B dashboard
# Look for signs:
# ‚Ä¢ Train loss < 0.3 but eval loss > 0.7 ‚Üí OVERFITTING
# ‚Ä¢ Eval loss starts increasing after epoch 1
# ‚Ä¢ Train/eval gap > 0.4

# Check logs
tail -100 output/experiments/*/logs/*.log
```

**2. Solution: Use conservative configs**

If you see overfitting, retrain with:
- `config/experiments/baseline_conservative.yaml` (rank=8, lr=1e-4)
- `config/experiments/polychromic_conservative.yaml` (Œª=0.15, N=2)

**3. When to use conservative configs:**
- Dataset < 1000 pairs
- Train/eval loss gap > 0.4
- Early stopping triggers at epoch 1  
- Model memorizing training examples

**See `docs/implementation/HYPERPARAMETER_STRATEGY.md` for detailed troubleshooting guide.**

---

#### **Day 7: Backup & Download Results**

**If on RunPod:**
```bash
# Download models and results
mkdir -p ~/Downloads/qwen_results
scp -r runpod:/workspace/Qwen3-8/output ~/Downloads/qwen_results/

# Download to your Mac
# output/experiments/baseline/seed_42/
# output/experiments/polychromic_0.3/seed_42/
```

**Verify you have:**
- [ ] Baseline adapter weights
- [ ] Polychromic adapter weights  
- [ ] Both diversity_dynamics.json files
- [ ] W&B run logs

---

### **Week 3: Analysis & Figure Generation**

**This is where all your novel contributions come to life!**

#### **Day 8: Standard Evaluation (2-3 hours)**

```bash
cd /Users/sohailmo/Repos/experiments/cuda/Qwen3-8
source qwen-lora-env/bin/activate

# Run comprehensive evaluation
python scripts/evaluation/evaluate_comprehensive.py \
  --baseline-lora output/experiments/baseline/seed_42 \
  --polychromic-lora output/experiments/polychromic_0.3/seed_42 \
  --test-data data/processed/test_data.jsonl \
  --output output/evaluation/ \
  --n-generations 10 \
  --max-examples 500 \
  --anthropic-key $ANTHROPIC_API_KEY

# Expected runtime: 1-2 hours
# Cost: ~$80 for LLM-judge
```

**Generates:**
- `output/evaluation/diversity_metrics.json`
- `output/evaluation/quality_metrics.json`
- `output/evaluation/passk_results.json` ‚Üê **Critical for novel metrics**
- `output/evaluation/llm_judge_results.json`
- `output/evaluation/statistical_tests.json`
- `output/evaluation/pareto_frontier.pdf` ‚Üí **Paper Figure 3** ‚úÖ

**‚úÖ Checkpoint: Standard evaluation complete!**

---

#### **Day 9: Novel Analysis - Part 1 (30 minutes)**

**Analysis 1: Diversity Dynamics Comparison**

```bash
python scripts/analysis/analyze_dynamics.py \
  --baseline output/experiments/baseline/seed_42/diversity_dynamics.json \
  --polychromic output/experiments/polychromic_0.3/seed_42/diversity_dynamics.json \
  --output output/figures/diversity_dynamics_comparison.pdf \
  --metric distinct_2

# Also try other metrics:
python scripts/analysis/analyze_dynamics.py \
  --baseline output/experiments/baseline/seed_42/diversity_dynamics.json \
  --polychromic output/experiments/polychromic_0.3/seed_42/diversity_dynamics.json \
  --output output/figures/diversity_dynamics_self_bleu.pdf \
  --metric self_bleu
```

**Generates:**
- `output/figures/diversity_dynamics_comparison.pdf` ‚Üí **Paper Figure 2** ‚≠ê NOVEL

**Finding to note:**
"Baseline diversity collapses from X to Y by step Z; Polychromic maintains > 0.70 throughout"

---

**Analysis 2: LoRA Parameter Analysis**

```bash
python scripts/analysis/analyze_lora_parameters.py \
  --baseline output/experiments/baseline/seed_42 \
  --polychromic output/experiments/polychromic_0.3/seed_42 \
  --output output/analysis/lora_comparison
```

**Generates:**
- `output/analysis/lora_comparison/lora_comparison.pdf` ‚Üí **Paper Figure 4** ‚≠ê NOVEL
- `output/analysis/lora_comparison/baseline_lora_heatmap.pdf`
- `output/analysis/lora_comparison/polychromic_lora_heatmap.pdf`
- `output/analysis/lora_comparison/comparison_summary.json`

**Finding to note:**
"Polychromic affects layers X-Y most (Z% higher update magnitude), suggesting diversity is encoded in late layers"

**‚úÖ Checkpoint: Novel analyses 1-2 complete!**

---

#### **Day 9 (continued): Novel Analysis - Part 2 (15 minutes)**

**Analysis 3: Novel Metrics**

```bash
python scripts/evaluation/evaluate_novel_metrics.py \
  --results-dir output/evaluation/ \
  --output output/evaluation/novel_metrics
```

**Generates:**
- `output/evaluation/novel_metrics/der_comparison.pdf` ‚Üí **Paper Figure 5** ‚≠ê NOVEL
- `output/evaluation/novel_metrics/collapse_points.pdf` ‚Üí **Paper Figure 6** ‚≠ê NOVEL
- `output/evaluation/novel_metrics/diversity_efficiency_analysis.json` ‚Üí **Paper Tables 5-7**

**Findings to note:**
- "Polychromic achieves DER@10 = X vs Y for baseline (Z√ó improvement)"
- "Collapse point: Polychromic useful up to k=N vs k=M for baseline"

**‚úÖ Checkpoint: All analyses complete! All 6 figures generated!**

---

#### **Day 10: Review and Organize Results**

**Verify all outputs:**

```bash
# Check you have all figures
ls -lh output/figures/
# - diversity_dynamics_comparison.pdf ‚Üê Figure 2

ls -lh output/evaluation/
# - pareto_frontier.pdf ‚Üê Figure 3

ls -lh output/analysis/lora_comparison/
# - lora_comparison.pdf ‚Üê Figure 4

ls -lh output/evaluation/novel_metrics/
# - der_comparison.pdf ‚Üê Figure 5
# - collapse_points.pdf ‚Üê Figure 6

# Count: Should have 6 PDFs total (excluding method diagram)
```

**Create organized folder for paper:**

```bash
mkdir -p paper/figures
mkdir -p paper/data

# Copy figures with paper names
cp output/figures/diversity_dynamics_comparison.pdf paper/figures/figure2_diversity_dynamics.pdf
cp output/evaluation/pareto_frontier.pdf paper/figures/figure3_pareto_frontier.pdf
cp output/analysis/lora_comparison/lora_comparison.pdf paper/figures/figure4_lora_analysis.pdf
cp output/evaluation/novel_metrics/der_comparison.pdf paper/figures/figure5_der_comparison.pdf
cp output/evaluation/novel_metrics/collapse_points.pdf paper/figures/figure6_collapse_points.pdf

# Copy numerical results
cp output/evaluation/*.json paper/data/
cp output/analysis/lora_comparison/*.json paper/data/
cp output/evaluation/novel_metrics/*.json paper/data/
```

**‚úÖ Checkpoint: All results organized for paper!**

---

### **Week 4-6: Paper Writing (3 weeks)**

**üìñ ESSENTIAL READING BEFORE WRITING:**

Before starting to write, read these strategic guides:

1. **`docs/research/STRATEGIC_POSITIONING_SUMMARY.md`** ‚≠ê CRITICAL
   - How to position vs. Pass@k Training, BA-LoRA, GEM
   - One-sentence pitches for abstract/intro
   - What to emphasize in each section
   
2. **`docs/research/novelty_research_claude.md`** ‚≠ê CRITICAL
   - Detailed competitive analysis
   - Related work section structure
   - Key differentiators to highlight

These guides tell you HOW to frame your contributions competitively!

---

#### **Week 4: Draft Core Sections**

**Day 11-12: Section 5.2 - Diversity Dynamics Analysis** ‚≠ê NOVEL

**What to write:**
```latex
\subsection{Diversity Dynamics Analysis}

To understand \textit{when} and \textit{why} diversity changes during 
training, we track diversity metrics at every 100 training steps. 
Figure~\ref{fig:dynamics} shows the evolution of Distinct-2 scores.

\begin{figure}[t]
\centering
\includegraphics[width=0.9\linewidth]{figures/figure2_diversity_dynamics.pdf}
\caption{Diversity dynamics during training. Baseline exhibits rapid 
diversity collapse after step 500 (Distinct-2 drops from 0.65 to 0.45), 
while polychromic maintains diversity above 0.70 throughout training.}
\label{fig:dynamics}
\end{figure}

We observe that baseline training exhibits rapid diversity collapse...
[Continue with your findings from diversity_dynamics.json]

Key insight: Explicit diversity regularization successfully counteracts 
the mode-collapse tendency of supervised fine-tuning.
```

**Data source:** `output/figures/diversity_dynamics_comparison.pdf` + JSON files

---

**Day 13-14: Section 5.4 - LoRA Parameter Analysis** ‚≠ê NOVEL

**What to write:**
```latex
\subsection{LoRA Parameter Analysis}

To understand \textit{where} diversity is encoded in the model, we analyze 
LoRA parameters layer-by-layer. Figure~\ref{fig:lora} shows the magnitude 
of LoRA updates ($||B \cdot A||$) across all layers.

\begin{figure}[t]
\centering
\includegraphics[width=0.9\linewidth]{figures/figure4_lora_analysis.pdf}
\caption{Layer-wise LoRA parameter analysis. Polychromic training shows 
23\% higher update magnitude in layers 24-36, suggesting diversity is 
primarily encoded in late transformer layers.}
\label{fig:lora}
\end{figure}

Analysis reveals that polychromic training primarily affects layers 24-36...
[Continue with findings from comparison_summary.json]

This suggests diversity is encoded in late transformer layers that control 
output distribution, rather than early content-processing layers.
```

**Data source:** `output/analysis/lora_comparison/` + JSON files

---

**Day 15-16: Section 5.5 - Novel Evaluation Metrics** ‚≠ê NOVEL

**What to write:**
```latex
\subsection{Novel Evaluation Metrics}

Standard Pass@k assumes perfect selection (choosing best according to 
ground truth). In deployment, users select based on heuristics or 
reward models. We introduce three novel metrics:

\subsubsection{User Selection Quality (USQ)}
USQ measures quality of selected candidates using realistic selection...

\subsubsection{Diversity Efficiency Ratio (DER)}
We propose DER = Pass@k / (k √ó Pass@1) to quantify diversity benefit...

\begin{figure}[t]
\centering
\includegraphics[width=0.8\linewidth]{figures/figure5_der_comparison.pdf}
\caption{Diversity Efficiency Ratio comparison. Polychromic achieves 
2.2√ó higher DER@10 than baseline.}
\label{fig:der}
\end{figure}

\subsubsection{Collapse Point Analysis}
Figure~\ref{fig:collapse} shows diversity collapse points...

\begin{figure}[t]
\centering
\includegraphics[width=0.8\linewidth]{figures/figure6_collapse_points.pdf}
\caption{Diversity collapse points. Polychromic useful up to k=12; 
baseline only k=3.}
\label{fig:collapse}
\end{figure}
```

**Data source:** `output/evaluation/novel_metrics/` + JSON files

---

#### **Week 5: Complete Draft**

**Day 17-18: Other Sections**

- Section 5.1: Main results (Pass@k, quality, diversity)
- Section 5.3: Pareto frontier analysis
- Section 6: Discussion
- Section 1: Introduction (refine)
- Section 2: Related work (position vs. competitors)

**Day 19-20: Tables**

Create all tables from JSON files:
```bash
# Main results
cat output/evaluation/diversity_metrics.json
cat output/evaluation/quality_metrics.json
cat output/evaluation/passk_results.json

# Novel metrics
cat output/evaluation/novel_metrics/diversity_efficiency_analysis.json

# LoRA analysis
cat output/analysis/lora_comparison/comparison_summary.json

# Statistical tests
cat output/evaluation/statistical_tests.json
```

**Day 21: First complete draft**

---

#### **Week 6: Revision & Polish**

**Day 22-24: Internal review**
- Read through entire paper
- Check all figures are referenced
- Verify all tables are populated
- Ensure findings match data
- Check citations

**Day 25-27: Polish**
- Improve writing clarity
- Strengthen introduction
- Refine related work positioning
- Add limitations section
- Proofread

---

### **Week 7: Submission Preparation**

**Day 28-30: Pre-submission checklist**

- [ ] Abstract clearly states contributions
- [ ] All 6 figures included and referenced
- [ ] All 7 tables populated
- [ ] Novel contributions (5) clearly highlighted
- [ ] Positioning vs. competitors clear
- [ ] Reproducibility: Code/data availability statement
- [ ] Acknowledgments
- [ ] References complete

**Day 31: Submit to EMNLP 2026!** üéâ

---

## üìä Detailed Analysis Commands

### **Command 1: Standard Evaluation** (2 hours, $80)

```bash
python scripts/evaluation/evaluate_comprehensive.py \
  --baseline-lora output/experiments/baseline/seed_42 \
  --polychromic-lora output/experiments/polychromic_0.3/seed_42 \
  --test-data data/processed/test_data.jsonl \
  --output output/evaluation/ \
  --n-generations 10 \
  --max-examples 500 \
  --anthropic-key $ANTHROPIC_API_KEY
```

**What it does:**
1. Generates 10 candidates for each test example
2. Computes diversity metrics (Self-BLEU, Distinct-n, semantic)
3. Computes quality metrics (ROUGE, BERTScore)
4. Computes Pass@k for k=1,3,5,10
5. Runs LLM-as-judge comparison
6. Statistical significance tests
7. **Automatically generates Pareto frontier (Figure 3)**

**Expected output:**
```json
{
  "baseline_lora": {
    "diversity_metrics": {
      "self_bleu": 0.45,
      "distinct_2": 0.62
    },
    "quality_metrics": {
      "rouge_l": 0.35
    },
    "passk_results": {
      "1": 0.42,
      "10": 0.46
    }
  },
  "polychromic_lora": {
    "diversity_metrics": {
      "self_bleu": 0.28,
      "distinct_2": 0.78
    },
    "quality_metrics": {
      "rouge_l": 0.33
    },
    "passk_results": {
      "1": 0.40,
      "10": 0.68
    }
  }
}
```

---

### **Command 2: Diversity Dynamics** (5 minutes)

```bash
python scripts/analysis/analyze_dynamics.py \
  --baseline output/experiments/baseline/seed_42/diversity_dynamics.json \
  --polychromic output/experiments/polychromic_0.3/seed_42/diversity_dynamics.json \
  --output output/figures/diversity_dynamics_comparison.pdf \
  --metric distinct_2
```

**What it does:**
1. Loads diversity snapshots from both models
2. Compares how Distinct-2 evolved during training
3. Generates comparison plot
4. Prints summary statistics

**Expected output:**
```
Baseline:
  Initial distinct_2: 0.65
  Final distinct_2: 0.45
  Change: -0.20 (COLLAPSED)

Polychromic:
  Initial distinct_2: 0.68
  Final distinct_2: 0.72
  Change: +0.04 (MAINTAINED/IMPROVED)
```

‚Üí **Paper Figure 2** ‚≠ê

---

### **Command 3: LoRA Parameter Analysis** (10 minutes)

```bash
python scripts/analysis/analyze_lora_parameters.py \
  --baseline output/experiments/baseline/seed_42 \
  --polychromic output/experiments/polychromic_0.3/seed_42 \
  --output output/analysis/lora_comparison
```

**What it does:**
1. Loads LoRA adapter weights from both models
2. Computes norms, effective ranks, singular values per layer
3. Compares layer-by-layer differences
4. Generates heatmaps and comparison plots

**Expected output:**
```
üìä Summary Statistics:
  Common layers: 84
  Baseline mean norm: 2.345
  Polychromic mean norm: 2.678
  Mean difference: +0.333

üéØ Largest updates:
  Baseline:    layers.35.self_attn.v_proj (norm = 5.23)
  Polychromic: layers.34.self_attn.q_proj (norm = 6.12)

Layers where polychromic > baseline: 67/84 (79.8%)
```

‚Üí **Paper Figure 4** ‚≠ê

---

### **Command 4: Novel Metrics** (5 minutes)

```bash
python scripts/evaluation/evaluate_novel_metrics.py \
  --results-dir output/evaluation/ \
  --output output/evaluation/novel_metrics
```

**What it does:**
1. Loads Pass@k results from standard evaluation
2. Computes DER for all k values
3. Finds collapse points
4. Generates comparison visualizations

**Expected output:**
```
Baseline:
  Collapse Point: k=3
  DER@5: 0.107 (poor)
  DER@10: 0.055 (poor)

Polychromic:
  Collapse Point: k=10
  DER@5: 0.310 (good)
  DER@10: 0.170 (moderate)
```

‚Üí **Paper Figures 5-6** ‚≠ê  
‚Üí **Paper Tables 5-7**

---

## üìù Data Extraction for Paper

### **For Each Figure:**

**Figure 2: Diversity Dynamics**
```bash
# Extract key numbers
cat output/experiments/baseline/seed_42/diversity_dynamics.json | \
  jq '.snapshots | [.[0], .[-1]] | .[] | {step, distinct_2}'

cat output/experiments/polychromic_0.3/seed_42/diversity_dynamics.json | \
  jq '.snapshots | [.[0], .[-1]] | .[] | {step, distinct_2}'
```

**Figure 3: Pareto Frontier**
```bash
cat output/evaluation/pareto_analysis.json | jq '.pareto_models'
```

**Figure 4: LoRA Analysis**
```bash
cat output/analysis/lora_comparison/comparison_summary.json | jq '.'
```

**Figures 5-6: Novel Metrics**
```bash
cat output/evaluation/novel_metrics/diversity_efficiency_analysis.json | jq '.'
```

---

### **For All Tables:**

**Table 1: Main Results**
```bash
# Combine all metrics
jq -s '.[0] as $div | .[1] as $qual | .[2] as $pass | 
  {baseline: {diversity: $div.baseline_lora, quality: $qual.baseline_lora, passk: $pass.baseline_lora},
   polychromic: {diversity: $div.polychromic_lora, quality: $qual.polychromic_lora, passk: $pass.polychromic_lora}}' \
  output/evaluation/diversity_metrics.json \
  output/evaluation/quality_metrics.json \
  output/evaluation/passk_results.json
```

**Tables 5-7: Novel Metrics**
```bash
cat output/evaluation/novel_metrics/diversity_efficiency_analysis.json | \
  jq '.[] | {collapse_point, der}'
```

---

## üéØ Paper Writing Checklist

### **Introduction:**
- [ ] State the training-deployment gap problem
- [ ] Motivate multi-candidate generation
- [ ] Preview 5 novel contributions
- [ ] State main findings (X% improvement in Pass@10)

### **Method (Section 3):**
- [ ] Describe diversity-aware training
- [ ] Loss formulation: L = L_quality - Œª¬∑D(generations)
- [ ] Training procedure
- [ ] Hyperparameters (from configs)

### **Experimental Setup (Section 4):**
- [ ] Data collection process
- [ ] Quality filters
- [ ] Four baselines
- [ ] Evaluation metrics (standard + novel)

### **Results & Analysis (Section 5) - MAIN FOCUS:**

**5.1: Main Results**
- [ ] Table 1: All metrics for all models
- [ ] Pass@k comparison (baseline vs. polychromic)
- [ ] Statistical significance (p-values, effect sizes)

**5.2: Diversity Dynamics** ‚≠ê NOVEL
- [ ] Figure 2: Diversity trajectory
- [ ] Finding: When diversity collapses
- [ ] Interpretation: Why polychromic maintains diversity

**5.3: Pareto Frontier**
- [ ] Figure 3: Quality-diversity trade-off
- [ ] Finding: Which models are Pareto-optimal
- [ ] Interpretation: Trade-off analysis

**5.4: LoRA Parameter Analysis** ‚≠ê NOVEL
- [ ] Figure 4: Layer-wise heatmaps
- [ ] Finding: Which layers encode diversity
- [ ] Interpretation: Late layers control output distribution

**5.5: Novel Evaluation Metrics** ‚≠ê NOVEL
- [ ] Introduce USQ, DER, Collapse Point
- [ ] Figure 5: DER comparison
- [ ] Figure 6: Collapse points
- [ ] Tables 5-7: Numerical results
- [ ] Interpretation: Practical implications

### **Discussion (Section 6):**
- [ ] Mechanistic insights from dynamics + LoRA analysis
- [ ] Practical implications of novel metrics
- [ ] Comparison with Pass@k Training, BA-LoRA, GEM
- [ ] When to use polychromic training
- [ ] Limitations

### **Conclusion (Section 7):**
- [ ] Summarize 5 contributions
- [ ] State main findings
- [ ] Future work (GRPO, cross-domain)

---

## üìã Pre-Submission Checklist

### **Content Complete:**
- [ ] All sections written (1-7)
- [ ] All 6 figures included
- [ ] All 7 tables populated
- [ ] Abstract summarizes contributions
- [ ] Related work positions vs. competitors

### **Quality Check:**
- [ ] Findings match data
- [ ] All claims supported by evidence
- [ ] Statistical tests reported correctly
- [ ] Figures are publication-quality
- [ ] No typos or grammar errors

### **Reproducibility:**
- [ ] Code availability statement
- [ ] Data availability statement (or privacy justification)
- [ ] Hyperparameters documented
- [ ] Random seeds specified

### **Formatting:**
- [ ] Follows EMNLP style guide
- [ ] References formatted correctly
- [ ] Figures/tables numbered correctly
- [ ] Appendix if needed

### **Ethics:**
- [ ] Limitations section
- [ ] Broader impact statement
- [ ] Data ethics discussed
- [ ] No harmful use cases

---

## üí∞ Complete Cost Breakdown

| Item | Time | Cost | When |
|------|------|------|------|
| Data curation | 2-3 days | $0 | Week 1 |
| Baseline training | 4 hrs | $3 | Week 2 |
| Polychromic training | 12 hrs | $9 | Week 2 |
| Standard evaluation | 2 hrs | $80 | Week 3 |
| Novel analyses | 1 hr | $0 | Week 3 |
| Paper writing | 3 weeks | $0 | Week 4-6 |
| Polish & submit | 1 week | $0 | Week 7 |
| **TOTAL** | **7 weeks** | **$132** | **Total** |

**Affordable for exceptional publication!**

---

## üéØ Decision Points

### **After Week 2 (Training Complete):**

**Check results:**
```bash
# View training curves on W&B
# Check: Did diversity maintain for polychromic?
# Check: Did both models converge?
```

**Decision:**
- ‚úÖ **If good:** Continue to analysis (Week 3)
- ‚ö†Ô∏è **If poor:** Retrain with adjusted hyperparameters

---

### **After Week 3 (Analysis Complete):**

**Review all 6 figures:**
```bash
ls paper/figures/
# figure2_diversity_dynamics.pdf
# figure3_pareto_frontier.pdf
# figure4_lora_analysis.pdf
# figure5_der_comparison.pdf
# figure6_collapse_points.pdf
```

**Decision:**
- ‚úÖ **If strong results:** Start writing (Week 4)
- ‚ö†Ô∏è **If weak:** Consider adding GRPO or more seeds

---

### **After Week 6 (Draft Complete):**

**Self-review:**
- Are all 5 contributions clear?
- Do results support claims?
- Is positioning vs. competitors clear?

**Decision:**
- ‚úÖ **If yes:** Polish and submit (Week 7)
- ‚ö†Ô∏è **If uncertain:** Get feedback, revise

---

## üìñ Documents You'll Need

### **During Training (Week 2):**
- Config files: `config/experiments/*.yaml`
- Training script: `scripts/training/train_model.py`
- Monitor: W&B dashboard

### **During Analysis (Week 3):**
- `QUICK_START_ANALYSIS.md` ‚Üê **Main guide**
- Analysis scripts in `scripts/analysis/` and `scripts/evaluation/`
- This workflow document

### **During Writing (Week 4-6):**
- All JSON files in `output/evaluation/` and `output/analysis/`
- All PDF figures in `paper/figures/`
- `docs/research/STRATEGIC_POSITIONING_SUMMARY.md` for positioning
- `docs/research/novelty_research_claude.md` for competitive analysis

---

## üö® Troubleshooting

### **If Training Fails:**

**Check:**
```bash
tail -100 output/experiments/*/logs/*.log
# Look for error messages
```

**Common issues:**
- OOM error ‚Üí Reduce batch size in config
- Loss not decreasing ‚Üí Check data quality
- Diversity collapsing (polychromic) ‚Üí Increase Œª

**Fix and rerun:**
```bash
# Modify config as needed
vim config/experiments/polychromic_0.3.yaml
# Retrain
python scripts/training/train_model.py --config ...
```

---

### **If Analysis Scripts Fail:**

**Check prerequisites:**
```bash
# Do you have diversity_dynamics.json?
ls output/experiments/*/seed_42/diversity_dynamics.json

# Do you have adapter weights?
ls output/experiments/*/seed_42/adapter_model.*

# Do you have Pass@k results?
ls output/evaluation/passk_results.json
```

**If missing:**
- Missing dynamics: Training didn't complete or `track_dynamics: false`
- Missing adapters: Training failed
- Missing Pass@k: Run evaluate_comprehensive.py first

---

### **If Figures Look Wrong:**

**Regenerate with different settings:**
```bash
# Try different metrics
python scripts/analysis/analyze_dynamics.py ... --metric self_bleu

# Adjust figure size or style in code if needed
```

---

## ‚úÖ Success Criteria

### **After Training:**
- ‚úÖ Baseline converged (eval_loss < 0.5)
- ‚úÖ Polychromic converged (combined_loss decreasing)
- ‚úÖ Polychromic diversity maintained (> 0.3)
- ‚úÖ Both models generate coherent text

### **After Analysis:**
- ‚úÖ All 6 figures generated
- ‚úÖ All JSON results files present
- ‚úÖ Polychromic shows higher diversity metrics
- ‚úÖ Statistical significance achieved (p < 0.05)

### **After Writing:**
- ‚úÖ 5 novel contributions clearly stated
- ‚úÖ All figures referenced in text
- ‚úÖ All tables populated with data
- ‚úÖ Positioning vs. competitors clear
- ‚úÖ Ready for submission

---

## üéä Summary

**This workflow takes you from:**
- Data curation complete ‚Üí Paper submitted to EMNLP 2026

**In 7 weeks with:**
- 5 novel scientific contributions
- 6 publication-quality figures
- 7 data-rich tables
- Strong positioning vs. competitors
- $132 total cost

**All steps are clearly documented. All code is tested. All analyses are automated.**

**You're ready! Just follow this workflow step-by-step!** üöÄ

---

## üìû Quick Help

**Where am I?** ‚Üí `IMPLEMENTATION_CHECKLIST.md`  
**How do I run analyses?** ‚Üí `QUICK_START_ANALYSIS.md`  
**What did I implement?** ‚Üí `FINAL_IMPLEMENTATION_SUMMARY.md`  
**Complete workflow?** ‚Üí **This document** ‚≠ê

---

**Your complete guide from data to paper. Follow step-by-step! üéØ‚ú®**

