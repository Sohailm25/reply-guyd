# GRPO Training - Phase 2 (from Baseline Warm-start)
#
# This config continues training from a baseline SFT checkpoint using GRPO
# (Group Relative Policy Optimization) to optimize for engagement.
#
# Scientific rationale:
# - Warm-start from baseline SFT checkpoint (Phase 1)
# - Uses Phase 2 data (non-overlapping with Phase 1)
# - Lower learning rate (1e-5 vs 2e-4) for fine-tuning
# - Reference model = Phase 1 checkpoint (prevents excessive drift)
# - Tests: Does GRPO improve over baseline SFT?
#
# Training paradigm:
#   Phase 1: Baseline SFT (match references)
#   Phase 2: GRPO (optimize engagement via RL)
#
# Usage:
#   python scripts/training/train_model.py \
#     --config config/experiments/grpo_from_baseline.yaml \
#     --checkpoint output/experiments/baseline_warmstart/seed_42

_base_: ../lora_config.yaml

# Experiment metadata
experiment:
  name: "grpo_from_baseline"
  phase: 2
  description: "GRPO refinement from baseline SFT warm-start"
  purpose: "Test if GRPO improves engagement over baseline SFT"
  warm_start_from: "baseline_warmstart"

# Model (inherits from base)
model:
  # Same as base config
  # Will load from Phase 1 checkpoint

# LoRA (inherits from base)
lora:
  # Same as base config

# Training - GRPO-specific adjustments
training:
  # Enable GRPO mode
  use_grpo: true
  
  # Checkpoint to load (Phase 1 baseline)
  checkpoint_path: "./output/experiments/baseline_warmstart/seed_42"
  
  # GRPO needs lower learning rate (fine-tuning)
  learning_rate: 1.0e-5  # 20x lower than SFT!
  
  # Longer warmup for stability
  warmup_ratio: 0.05  # vs 0.03 for SFT
  
  # More epochs for GRPO (RL needs more iterations)
  num_epochs: 3
  
  # Weight decay and gradient clipping
  weight_decay: 0.01
  max_grad_norm: 1.0
  
  # Batch sizes - more gradient accumulation for stability
  per_device_train_batch_size: 2  # Smaller due to multiple generations
  per_device_eval_batch_size: 2
  gradient_accumulation_steps: 8  # Effective batch size = 16
  
  # Optimizer
  optim: "adamw_torch"
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1.0e-8
  
  # Learning rate scheduler
  lr_scheduler_type: "cosine"
  
  # Mixed precision
  bf16: true
  fp16: false
  
  # Gradient checkpointing
  gradient_checkpointing: true
  
  # Reward model path
  reward_model_path: "./output/models/reward_model"  # If using learned reward
  reward_type: "heuristic"  # Options: "heuristic", "learned", "hybrid"

# GRPO-specific configuration
grpo:
  # Number of generations per prompt for GRPO
  n_generations: 4
  
  # KL divergence coefficient (Î²)
  # Controls how much model can drift from reference
  kl_coeff: 0.1
  
  # Generation parameters
  temperature: 0.8  # Slightly lower than polychromic (0.9)
  top_p: 0.9
  max_new_tokens: 100
  
  # Advantage normalization
  clip_advantage: true
  advantage_clip_range: 10.0  # Prevents extreme advantages
  
  # Reference model settings
  reference_model_source: "checkpoint"  # Use Phase 1 checkpoint as reference
  freeze_reference: true  # Keep reference model frozen
  
  # Computational efficiency
  compute_grpo_every_n_steps: 1  # Every step (can reduce to 2 for speed)
  max_examples_per_batch: 4

# Reward function configuration
reward:
  # Heuristic reward components (if using heuristic)
  heuristic:
    relevance_weight: 0.30
    length_weight: 0.15
    sentiment_weight: 0.15
    uniqueness_weight: 0.10
    safety_weight: 0.10
    diversity_weight: 0.20
  
  # Learned reward model (if using learned)
  learned:
    model_path: "./output/models/reward_model"
    normalize_scores: true
  
  # Hybrid (if using both)
  hybrid:
    learned_weight: 0.6
    heuristic_weight: 0.4

# Data - Phase 2 specific
data:
  # Phase 2 data (generated by split_training_phases.py)
  train_file: "data/processed/phases/training_data_phase2_grpo.jsonl"
  
  max_length: 512
  seed: 42
  
  # Use standard train/eval split (90/10 of Phase 2 data)
  train_split: 0.9
  eval_split: 0.1
  
  # Qwen3 specific
  enable_thinking: false
  add_generation_prompt: false

# Evaluation Strategy
evaluation:
  strategy: "steps"
  eval_steps: 50
  logging_steps: 10
  save_steps: 50
  save_total_limit: 3
  
  # Early stopping (more patience for GRPO)
  early_stopping:
    enabled: true
    patience: 5  # vs 3 for SFT (GRPO can be noisy)
    threshold: 0.01
  
  # Best model selection
  load_best_model_at_end: true
  metric_for_best_model: "eval_loss"
  greater_is_better: false

# Output
output:
  output_dir: "./output/experiments/grpo_from_baseline"
  logging_dir: "./output/logs/grpo_from_baseline"
  save_safetensors: true
  save_only_model: false

# Weights & Biases
wandb:
  enabled: true
  project: "qwen3-twitter-grpo"
  name: "grpo-from-baseline-phase2"
  tags:
    - "phase2"
    - "grpo"
    - "reinforcement_learning"
    - "baseline_warmstart"
    - "engagement_optimization"
  notes: "Phase 2: GRPO refinement from baseline SFT warm-start to optimize engagement"
  
  # Logging
  log_model: true
  watch: "all"

# Reproducibility
reproducibility:
  seed: 42
  deterministic: false

# GRPO-specific monitoring thresholds
grpo_monitoring:
  # KL divergence alerts
  max_kl_divergence: 15.0  # Warning if KL > 15
  target_kl_divergence: 5.0  # Target KL ~5
  
  # Reward tracking
  min_avg_reward: 0.3  # Warning if avg reward < 0.3
  
  # Generation quality checks
  min_generation_length: 10  # Flag if avg length < 10 chars
  max_generation_length: 280  # Flag if avg length > 280 chars


