# GRPO Training - Phase 2 (from Polychromic Warm-start)
#
# This config continues training from a polychromic SFT checkpoint using GRPO
# (Group Relative Policy Optimization) to optimize for engagement.
#
# Scientific rationale:
# - Warm-start from polychromic SFT checkpoint (Phase 1)
# - Hypothesis: Diverse warm-start improves GRPO exploration
# - Uses Phase 2 data (non-overlapping with Phase 1)
# - Lower learning rate (1e-5 vs 2e-4) for fine-tuning
# - Reference model = Phase 1 checkpoint (prevents excessive drift)
#
# Training paradigm:
#   Phase 1: Polychromic SFT (match references + diversity)
#   Phase 2: GRPO (optimize engagement via RL)
#   Expected: Best overall performance (quality + diversity + engagement)
#
# This is the main research contribution:
#   Supervised + Polychromic + GRPO
#
# Usage:
#   python scripts/training/train_model.py \
#     --config config/experiments/grpo_from_polychromic.yaml \
#     --checkpoint output/experiments/polychromic_warmstart/seed_42

_base_: ../lora_config.yaml

# Experiment metadata
experiment:
  name: "grpo_from_polychromic"
  phase: 2
  description: "GRPO refinement from polychromic SFT warm-start"
  purpose: "Test synergy: Does diverse warm-start improve GRPO performance?"
  warm_start_from: "polychromic_warmstart"
  contribution: "main"  # This is our primary contribution

# Model (inherits from base)
model:
  # Same as base config
  # Will load from Phase 1 polychromic checkpoint

# LoRA (inherits from base)
lora:
  # Same as base config

# Training - GRPO-specific adjustments
training:
  # Enable GRPO mode
  use_grpo: true
  
  # Checkpoint to load (Phase 1 polychromic)
  checkpoint_path: "./output/experiments/polychromic_warmstart/seed_42"
  
  # GRPO needs lower learning rate (fine-tuning)
  learning_rate: 1.0e-5  # 20x lower than SFT!
  
  # Longer warmup for stability
  warmup_ratio: 0.05  # vs 0.03 for SFT
  
  # More epochs for GRPO (RL needs more iterations)
  num_epochs: 3
  
  # Weight decay and gradient clipping
  weight_decay: 0.01
  max_grad_norm: 1.0
  
  # Batch sizes - more gradient accumulation for stability
  per_device_train_batch_size: 2  # Smaller due to multiple generations
  per_device_eval_batch_size: 2
  gradient_accumulation_steps: 8  # Effective batch size = 16
  
  # Optimizer
  optim: "adamw_torch"
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1.0e-8
  
  # Learning rate scheduler
  lr_scheduler_type: "cosine"
  
  # Mixed precision
  bf16: true
  fp16: false
  
  # Gradient checkpointing
  gradient_checkpointing: true
  
  # Reward model path
  reward_model_path: "./output/models/reward_model"  # If using learned reward
  reward_type: "heuristic"  # Options: "heuristic", "learned", "hybrid"

# GRPO-specific configuration
grpo:
  # Number of generations per prompt for GRPO
  n_generations: 4
  
  # KL divergence coefficient (Î²)
  # May need slight adjustment for polychromic start (model already diverse)
  kl_coeff: 0.1  # Start with same as baseline, tune if needed
  
  # Generation parameters
  temperature: 0.8  # Slightly lower than polychromic (0.9)
  top_p: 0.9
  max_new_tokens: 100
  
  # Advantage normalization
  clip_advantage: true
  advantage_clip_range: 10.0  # Prevents extreme advantages
  
  # Reference model settings
  reference_model_source: "checkpoint"  # Use Phase 1 polychromic checkpoint
  freeze_reference: true  # Keep reference model frozen
  
  # Computational efficiency
  compute_grpo_every_n_steps: 1  # Every step (can reduce to 2 for speed)
  max_examples_per_batch: 4
  
  # Diversity maintenance (optional)
  # Since we start from polychromic, we can add diversity bonus to GRPO
  maintain_diversity: true
  diversity_bonus_weight: 0.1  # Small bonus for diverse generations

# Reward function configuration
reward:
  # Heuristic reward components (if using heuristic)
  heuristic:
    relevance_weight: 0.30
    length_weight: 0.15
    sentiment_weight: 0.15
    uniqueness_weight: 0.10
    safety_weight: 0.10
    diversity_weight: 0.20  # Higher weight since we value diversity
  
  # Learned reward model (if using learned)
  learned:
    model_path: "./output/models/reward_model"
    normalize_scores: true
  
  # Hybrid (if using both)
  hybrid:
    learned_weight: 0.6
    heuristic_weight: 0.4

# Data - Phase 2 specific
data:
  # Phase 2 data (generated by split_training_phases.py)
  train_file: "data/processed/phases/training_data_phase2_grpo.jsonl"
  
  max_length: 512
  seed: 42
  
  # Use standard train/eval split (90/10 of Phase 2 data)
  train_split: 0.9
  eval_split: 0.1
  
  # Qwen3 specific
  enable_thinking: false
  add_generation_prompt: false

# Evaluation Strategy
evaluation:
  strategy: "steps"
  eval_steps: 50
  logging_steps: 10
  save_steps: 50
  save_total_limit: 3
  
  # Early stopping (more patience for GRPO)
  early_stopping:
    enabled: true
    patience: 5  # vs 3 for SFT (GRPO can be noisy)
    threshold: 0.01
  
  # Best model selection
  load_best_model_at_end: true
  metric_for_best_model: "eval_loss"
  greater_is_better: false

# Output
output:
  output_dir: "./output/experiments/grpo_from_polychromic"
  logging_dir: "./output/logs/grpo_from_polychromic"
  save_safetensors: true
  save_only_model: false

# Weights & Biases
wandb:
  enabled: true
  project: "qwen3-twitter-grpo"
  name: "grpo-from-polychromic-phase2"
  tags:
    - "phase2"
    - "grpo"
    - "reinforcement_learning"
    - "polychromic_warmstart"
    - "engagement_optimization"
    - "main_contribution"
  notes: |
    Phase 2: GRPO refinement from polychromic SFT warm-start.
    
    This is our main research contribution:
    - Phase 1: Supervised learning + diversity regularization
    - Phase 2: Reinforcement learning for engagement optimization
    - Hypothesis: Combination achieves best Pass@k and engagement
  
  # Logging
  log_model: true
  watch: "all"

# Reproducibility
reproducibility:
  seed: 42
  deterministic: false

# GRPO-specific monitoring thresholds
grpo_monitoring:
  # KL divergence alerts
  max_kl_divergence: 15.0  # Warning if KL > 15
  target_kl_divergence: 5.0  # Target KL ~5
  
  # Reward tracking
  min_avg_reward: 0.3  # Warning if avg reward < 0.3
  
  # Generation quality checks
  min_generation_length: 10  # Flag if avg length < 10 chars
  max_generation_length: 280  # Flag if avg length > 280 chars
  
  # Diversity monitoring (important for polychromic start)
  min_diversity_score: 0.25  # Warning if diversity drops below Phase 1 level

